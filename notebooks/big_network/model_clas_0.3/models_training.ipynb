{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from models.train import train, test\n",
    "from models.models import GCN, GAT, GraphSAGE, FNN\n",
    "from models.utils import ContagionDataset, set_seed\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(ld, indent=0):\n",
    "    return None\n",
    "    with open('result.txt', 'w', encoding='utf-8') as file:\n",
    "        for d in tqdm(ld):\n",
    "            file.write('{' + '\\n')\n",
    "            for key, value in d.items():\n",
    "                file.write('\\t' * (indent+1) + str(key) + ':' + str(value) + '\\n')\n",
    "                # file.write('\\t' * (indent+1) + str(key) + '\\n')\n",
    "                # file.write('\\t' * (indent+2) + str(value) + '\\n')\n",
    "            file.write('},\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = True\n",
    "\n",
    "seed = 4444\n",
    "set_seed(seed)\n",
    "\n",
    "metric_filter_1 = 'test_mcc'\n",
    "metric_filter_2 = 'val_mcc'\n",
    "\n",
    "data_dir = '../data'\n",
    "log_path = './logs'\n",
    "save_path = './saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big dataset: Additional stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_lengths = (0.2, 0.1, 0.7)\n",
    "target = 'additional_stress'\n",
    "\n",
    "dataset = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "out_feats = dataset.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_small_acc_train = {}\n",
    "dict_small_acc_val = {}\n",
    "dict_small_acc_test = {}\n",
    "dict_small_rmse_train = {}\n",
    "dict_small_rmse_val = {}\n",
    "dict_small_rmse_test = {}\n",
    "dict_small_mcc_train = {}\n",
    "dict_small_mcc_val = {}\n",
    "dict_small_mcc_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train,y_test = train_test_split(dataset.node_features[0].to_numpy(), dataset.targets[0], test_size=0.25, random_state=seed)\n",
    "g_data = dataset.graphs[0].ndata\n",
    "feats = g_data['feat']\n",
    "labels = g_data['label']\n",
    "train_mask = g_data['train_mask']\n",
    "val_mask = g_data['val_mask']\n",
    "test_mask = g_data['test_mask']\n",
    "\n",
    "# train + val for training, test for test\n",
    "x_train,x_test = feats[torch.logical_not(test_mask)], feats[test_mask]\n",
    "y_train,y_test = labels[torch.logical_not(test_mask)], labels[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([450, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1050, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.69      0.73       256\n",
      "           1       0.31      0.07      0.11       272\n",
      "           2       0.33      0.44      0.38       266\n",
      "           3       0.48      0.76      0.59       256\n",
      "\n",
      "    accuracy                           0.48      1050\n",
      "   macro avg       0.48      0.49      0.45      1050\n",
      "weighted avg       0.47      0.48      0.45      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial',random_state=seed, max_iter=800).fit(x_train, y_train)\n",
    "print(classification_report(y_true=y_test, y_pred=model_lr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5088888888888888\n",
      "Test accuracy: 0.4838095238095238\n",
      "Train rmse: 0.943986817233753\n",
      "Test rmse: 1.0071175275436894\n",
      "Train mcc: 0.3505812598646003\n",
      "Test mcc: 0.3292826503614915\n"
     ]
    }
   ],
   "source": [
    "dict_small_acc_train['logistic_regression'] = model_lr.score(x_train, y_train)\n",
    "dict_small_acc_test['logistic_regression'] = model_lr.score(x_test, y_test)\n",
    "print(f\"Train accuracy: {dict_small_acc_train['logistic_regression']}\")\n",
    "print(f\"Test accuracy: {dict_small_acc_test['logistic_regression']}\")\n",
    "\n",
    "dict_small_rmse_train['logistic_regression'] = mean_squared_error(y_true=y_train,y_pred=model_lr.predict(x_train), squared=False)\n",
    "dict_small_rmse_test['logistic_regression'] = mean_squared_error(y_true=y_test,y_pred=model_lr.predict(x_test), squared=False)\n",
    "print(f\"Train rmse: {dict_small_rmse_train['logistic_regression']}\")\n",
    "print(f\"Test rmse: {dict_small_rmse_test['logistic_regression']}\")\n",
    "\n",
    "dict_small_mcc_train['logistic_regression'] = matthews_corrcoef(y_true=y_train,y_pred=model_lr.predict(x_train))\n",
    "dict_small_mcc_test['logistic_regression'] = matthews_corrcoef(y_true=y_test,y_pred=model_lr.predict(x_test))\n",
    "print(f\"Train mcc: {dict_small_mcc_train['logistic_regression']}\")\n",
    "print(f\"Test mcc: {dict_small_mcc_test['logistic_regression']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_rf,x_val_rf,y_train_rf,y_val_rf = train_test_split(x_train, y_train, test_size=0.2, random_state=seed)\n",
    "x_train_rf,x_val_rf,x_test_rf = feats[train_mask], feats[val_mask], feats[test_mask]\n",
    "y_train_rf,y_val_rf,y_test_rf = labels[train_mask], labels[val_mask], labels[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:03<00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.72      0.66       256\n",
      "           1       0.30      0.23      0.26       272\n",
      "           2       0.29      0.23      0.25       266\n",
      "           3       0.46      0.59      0.52       256\n",
      "\n",
      "    accuracy                           0.44      1050\n",
      "   macro avg       0.42      0.44      0.42      1050\n",
      "weighted avg       0.41      0.44      0.42      1050\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "num_nodes = x_train_rf.shape[0]\n",
    "model_rf = None\n",
    "val_acc = 0.0\n",
    "for k in trange(1,num_nodes, (num_nodes - 1) // n):\n",
    "    tmp = RandomForestClassifier(random_state=seed, n_estimators=k).fit(x_train_rf,y_train_rf)\n",
    "    tmp_acc = tmp.score(x_val_rf, y_val_rf)\n",
    "    if val_acc < tmp_acc:\n",
    "        val_acc = tmp_acc\n",
    "        model_rf = tmp\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=model_rf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=85, random_state=4444)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Val accuracy: 0.46\n",
      "Test accuracy: 0.4380952380952381\n",
      "Train rmse: 0.0\n",
      "Val rmse: 1.003327796219494\n",
      "Test rmse: 1.0915257997081753\n",
      "Train mcc: 1.0\n",
      "Val mcc: 0.28487271906114675\n",
      "Test mcc: 0.25440692539812415\n"
     ]
    }
   ],
   "source": [
    "dict_small_acc_train['random_forest'] = model_rf.score(x_train_rf, y_train_rf)\n",
    "dict_small_acc_val['random_forest'] = model_rf.score(x_val_rf, y_val_rf)\n",
    "dict_small_acc_test['random_forest'] = model_rf.score(x_test, y_test)\n",
    "print(f\"Train accuracy: {dict_small_acc_train['random_forest']}\")\n",
    "print(f\"Val accuracy: {dict_small_acc_val['random_forest']}\")\n",
    "print(f\"Test accuracy: {dict_small_acc_test['random_forest']}\")\n",
    "\n",
    "dict_small_rmse_train['random_forest'] = mean_squared_error(y_true=y_train_rf,y_pred=model_rf.predict(x_train_rf), squared=False)\n",
    "dict_small_rmse_val['random_forest'] = mean_squared_error(y_true=y_val_rf,y_pred=model_rf.predict(x_val_rf), squared=False)\n",
    "dict_small_rmse_test['random_forest'] = mean_squared_error(y_true=y_test,y_pred=model_rf.predict(x_test), squared=False)\n",
    "print(f\"Train rmse: {dict_small_rmse_train['random_forest']}\")\n",
    "print(f\"Val rmse: {dict_small_rmse_val['random_forest']}\")\n",
    "print(f\"Test rmse: {dict_small_rmse_test['random_forest']}\")\n",
    "\n",
    "dict_small_mcc_train['random_forest'] = matthews_corrcoef(y_true=y_train_rf,y_pred=model_rf.predict(x_train_rf))\n",
    "dict_small_mcc_val['random_forest'] = matthews_corrcoef(y_true=y_val_rf,y_pred=model_rf.predict(x_val_rf))\n",
    "dict_small_mcc_test['random_forest'] = matthews_corrcoef(y_true=y_test,y_pred=model_rf.predict(x_test))\n",
    "print(f\"Train mcc: {dict_small_mcc_train['random_forest']}\")\n",
    "print(f\"Val mcc: {dict_small_mcc_val['random_forest']}\")\n",
    "print(f\"Test mcc: {dict_small_mcc_test['random_forest']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.70      0.62       256\n",
      "           1       0.31      0.27      0.29       272\n",
      "           2       0.29      0.27      0.28       266\n",
      "           3       0.36      0.32      0.34       256\n",
      "\n",
      "    accuracy                           0.39      1050\n",
      "   macro avg       0.38      0.39      0.38      1050\n",
      "weighted avg       0.37      0.39      0.38      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=out_feats).fit(x_train,y_train)\n",
    "print(classification_report(y_true=y_test, y_pred=model_knn.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6333333333333333\n",
      "Test accuracy: 0.38857142857142857\n",
      "Train rmse: 0.9018499505645788\n",
      "Test rmse: 1.245945806357946\n",
      "Train mcc: 0.5103666721694549\n",
      "Test mcc: 0.18596284324734758\n"
     ]
    }
   ],
   "source": [
    "dict_small_acc_train['knn_classifier'] = model_knn.score(x_train_rf, y_train_rf)\n",
    "dict_small_acc_test['knn_classifier'] = model_knn.score(x_test, y_test)\n",
    "print(f\"Train accuracy: {dict_small_acc_train['knn_classifier']}\")\n",
    "print(f\"Test accuracy: {dict_small_acc_test['knn_classifier']}\")\n",
    "\n",
    "dict_small_rmse_train['knn_classifier'] = mean_squared_error(y_true=y_train_rf,y_pred=model_knn.predict(x_train_rf), squared=False)\n",
    "dict_small_rmse_test['knn_classifier'] = mean_squared_error(y_true=y_test,y_pred=model_knn.predict(x_test), squared=False)\n",
    "print(f\"Train rmse: {dict_small_rmse_train['knn_classifier']}\")\n",
    "print(f\"Test rmse: {dict_small_rmse_test['knn_classifier']}\")\n",
    "\n",
    "dict_small_mcc_train['knn_classifier'] = matthews_corrcoef(y_true=y_train_rf,y_pred=model_knn.predict(x_train_rf))\n",
    "dict_small_mcc_test['knn_classifier'] = matthews_corrcoef(y_true=y_test,y_pred=model_knn.predict(x_test))\n",
    "print(f\"Train mcc: {dict_small_mcc_train['knn_classifier']}\")\n",
    "print(f\"Test mcc: {dict_small_mcc_test['knn_classifier']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_fnn'\n",
    "log_dir = f'{log_path}_fnn'\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = False,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "# fnn_model = dict(\n",
    "#     in_features=[len(dataset_val.node_attributes)],\n",
    "#     h_features=[[5, 10], [10, 15], [5,5,5], [5, 10, 15], [5, 10, 15, 20], [5], [10], [15]],\n",
    "#     out_features=[dataset_val.num_classes],\n",
    "#     activation=[torch.nn.ReLU()],\n",
    "#     norm_nodes = [None, 'bn', 'gn'],\n",
    "#     dropout=[0.2, 0.5, 0.0],\n",
    "#     # other\n",
    "#     lr=[1, 1e-1, 1e-2],\n",
    "#     label_smoothing=[0.0, 0.2, 0.4],\n",
    "# )\n",
    "\n",
    "fnn_model = dict(\n",
    "    in_features=[len(dataset_val.node_attributes)],\n",
    "    h_features=[[10, 15], [10, 15, 20], [5, 10, 15], [15] * 2, [15] * 3],\n",
    "    out_features=[dataset_val.num_classes],\n",
    "    activation=[torch.nn.ReLU()],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    dropout=[0.2, 0.0],\n",
    "    # other\n",
    "    lr=[1e-1],\n",
    "    label_smoothing=[0.0, 0.2],\n",
    ")\n",
    "list_model = [dict(zip(fnn_model.keys(), k)) for k in itertools.product(*fnn_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:55<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        ls = d.pop('label_smoothing')\n",
    "\n",
    "        train(\n",
    "            model=FNN(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_val,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            label_smoothing=ls,\n",
    "            use_edge_weight=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1251/1251 [00:48<00:00, 25.65it/s]\n"
     ]
    }
   ],
   "source": [
    "res_edges_fnn = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [10, 15, 20],\n",
       " 'out_features': 4,\n",
       " 'activation': ReLU(),\n",
       " 'norm_nodes': 'bn',\n",
       " 'dropout': 0.0,\n",
       " 'train_lr': 0.1,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': False,\n",
       " 'train_self_loop': False,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 1.0108048,\n",
       " 'train_acc': 0.5066666603088379,\n",
       " 'val_acc': 0.5066666007041931,\n",
       " 'epoch': 93,\n",
       " 'model_class': 'fnn',\n",
       " 'path_name': '0.5066666007041931_4_[10_15_20]_4_ReLU()_bn_0.0_0.1_adamw_max_val_mcc_0.0_False_False_0',\n",
       " 'train_mcc': 0.36249149412957854,\n",
       " 'val_mcc': 0.3567429658664404,\n",
       " 'test_mcc': 0.3765097324418532,\n",
       " 'train_rmse': 0.8944271909999159,\n",
       " 'val_rmse': 1.055146119422961,\n",
       " 'test_rmse': 0.9739463170891035,\n",
       " 'test_acc': 0.5152381062507629}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_edges = res_edges_fnn\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [5, 10, 15],\n",
       " 'out_features': 4,\n",
       " 'activation': ReLU(),\n",
       " 'norm_nodes': 'bn',\n",
       " 'dropout': 0.0,\n",
       " 'train_lr': 0.1,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.2,\n",
       " 'train_use_edge_weight': False,\n",
       " 'train_self_loop': False,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 1.2271523,\n",
       " 'train_acc': 0.5099999904632568,\n",
       " 'val_acc': 0.48666661977767944,\n",
       " 'epoch': 55,\n",
       " 'model_class': 'fnn',\n",
       " 'path_name': '0.48666661977767944_4_[5_10_15]_4_ReLU()_bn_0.0_0.1_adamw_max_val_mcc_0.2_False_False_0',\n",
       " 'train_mcc': 0.374620744992361,\n",
       " 'val_mcc': 0.34501477641512174,\n",
       " 'test_mcc': 0.38317842940237273,\n",
       " 'train_rmse': 0.8812869377601523,\n",
       " 'val_rmse': 1.0099504938362078,\n",
       " 'test_rmse': 0.9118270512704542,\n",
       " 'test_acc': 0.5057142972946167}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [10, 15, 20],\n",
       " 'out_features': 4,\n",
       " 'activation': ReLU(),\n",
       " 'norm_nodes': 'bn',\n",
       " 'dropout': 0.0,\n",
       " 'train_lr': 0.1,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': False,\n",
       " 'train_self_loop': False,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 1.0131788,\n",
       " 'train_acc': 0.5199999809265137,\n",
       " 'val_acc': 0.5333333015441895,\n",
       " 'epoch': 67,\n",
       " 'model_class': 'fnn',\n",
       " 'path_name': '0.5333333015441895_4_[10_15_20]_4_ReLU()_bn_0.0_0.1_adamw_max_val_mcc_0.0_False_False_0',\n",
       " 'train_mcc': 0.37650807048789786,\n",
       " 'val_mcc': 0.4052364539874979,\n",
       " 'test_mcc': 0.36759820397823106,\n",
       " 'train_rmse': 0.8426149773176358,\n",
       " 'val_rmse': 0.9237604307034013,\n",
       " 'test_rmse': 0.8992059989632123,\n",
       " 'test_acc': 0.5019047856330872}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic_regression': 0.5088888888888888, 'random_forest': 1.0, 'knn_classifier': 0.6333333333333333}\n",
      "{'random_forest': 0.46}\n",
      "{'logistic_regression': 0.4838095238095238, 'random_forest': 0.4380952380952381, 'knn_classifier': 0.38857142857142857}\n"
     ]
    }
   ],
   "source": [
    "print(dict_small_acc_train)\n",
    "print(dict_small_acc_val)\n",
    "print(dict_small_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic_regression': 0.3505812598646003, 'random_forest': 1.0, 'knn_classifier': 0.5103666721694549}\n",
      "{'random_forest': 0.28487271906114675}\n",
      "{'logistic_regression': 0.3292826503614915, 'random_forest': 0.25440692539812415, 'knn_classifier': 0.18596284324734758}\n"
     ]
    }
   ],
   "source": [
    "print(dict_small_mcc_train)\n",
    "print(dict_small_mcc_val)\n",
    "print(dict_small_mcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic_regression': 0.943986817233753, 'random_forest': 0.0, 'knn_classifier': 0.9018499505645788}\n",
      "{'random_forest': 1.003327796219494}\n",
      "{'logistic_regression': 1.0071175275436894, 'random_forest': 1.0915257997081753, 'knn_classifier': 1.245945806357946}\n"
     ]
    }
   ],
   "source": [
    "print(dict_small_rmse_train)\n",
    "print(dict_small_rmse_val)\n",
    "print(dict_small_rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_gcn'\n",
    "log_dir = f'{log_path}_gcn'\n",
    "\n",
    "add_self_loop = True\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = add_self_loop,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "gcn_model = dict(\n",
    "    in_features=[len(dataset_val.node_attributes)],\n",
    "    h_features=[[15] * 5, [15] * 4, [10, 15, 20], [5, 10, 15, 20],],\n",
    "    out_features=[dataset_val.num_classes],\n",
    "    activation=[torch.nn.ReLU()],\n",
    "    norm_edges=['both', 'none'],\n",
    "    norm_nodes=[None, 'bn', 'gn'],\n",
    "    dropout=[0.2, 0.0],\n",
    "    # other\n",
    "    lr=[1e-1],\n",
    "    label_smoothing=[0.0, 0.2],\n",
    "    use_edge_weight=[True,],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(gcn_model.keys(), k)) for k in itertools.product(*gcn_model.values())]\n",
    "\n",
    "# gcn_model = dict(\n",
    "#     in_features=[len(dataset_val.node_attributes)],\n",
    "#     h_features=[[5, 10], [10, 15], [5,5,5], [5, 10, 15], [5, 10, 15, 20], [5], [10], [15]],\n",
    "#     # h_features=[[5, 10], [10, 15], [5], [10], [15], [10,15]],\n",
    "#     out_features=[dataset_val.num_classes],\n",
    "#     activation=[torch.nn.ReLU()],\n",
    "#     norm_edges=['both', 'none'],\n",
    "#     norm_nodes=[None, 'bn', 'gn'],\n",
    "#     dropout=[0.2, 0.5, 0.0],\n",
    "#     # other\n",
    "#     lr=[1],\n",
    "#     label_smoothing=[0.0, 0.2, 0.4],\n",
    "#     use_edge_weight=[True, False],\n",
    "#     drop_edges=[0,0.2,0.4],\n",
    "# )\n",
    "# list_model = [{i:j[k] for i,j in gcn_model.items()} for k in range(len(gcn_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [19:13<00:00,  6.01s/it]\n"
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        ls = d.pop('label_smoothing')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        # dataset_valid = ContagionDataset(\n",
    "        #     raw_dir=data_dir,\n",
    "        #     drop_edges=0,\n",
    "        #     sets_lengths=sets_lengths,\n",
    "        #     add_self_loop = add_self_loop,\n",
    "        #     target = target,\n",
    "        #     seed=seed,\n",
    "        # )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GCN(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            label_smoothing=ls,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3625/3625 [03:03<00:00, 19.72it/s]\n"
     ]
    }
   ],
   "source": [
    "res_edges_gcn = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [5, 10, 15, 20],\n",
       " 'out_features': 4,\n",
       " 'activation': ReLU(),\n",
       " 'norm_edges': 'none',\n",
       " 'norm_nodes': 'gn',\n",
       " 'dropout': 0.0,\n",
       " 'train_lr': 0.1,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': True,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 1.2598416,\n",
       " 'train_acc': 0.4566666781902313,\n",
       " 'val_acc': 0.5333333015441895,\n",
       " 'epoch': 13,\n",
       " 'model_class': 'gcn',\n",
       " 'path_name': '0.5333333015441895_4_[5_10_15_20]_4_ReLU()_none_gn_0.0_0.1_adamw_max_val_mcc_0.0_True_True_0',\n",
       " 'train_mcc': 0.27295628406392525,\n",
       " 'val_mcc': 0.38101377137885245,\n",
       " 'test_mcc': 0.4132204578621432,\n",
       " 'train_rmse': 1.0181682899534177,\n",
       " 'val_rmse': 0.9309493362512627,\n",
       " 'test_rmse': 0.9314607064278383,\n",
       " 'test_acc': 0.5600000023841858}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_edges = res_edges_gcn\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [15, 15, 15, 15],\n",
       " 'out_features': 4,\n",
       " 'activation': ReLU(),\n",
       " 'norm_edges': 'none',\n",
       " 'norm_nodes': 'bn',\n",
       " 'dropout': 0.0,\n",
       " 'train_lr': 0.1,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': True,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 1.3793306,\n",
       " 'train_acc': 0.47999998927116394,\n",
       " 'val_acc': 0.4666666090488434,\n",
       " 'epoch': 10,\n",
       " 'model_class': 'gcn',\n",
       " 'path_name': '0.4666666090488434_4_[15_15_15_15]_4_ReLU()_none_bn_0.0_0.1_adamw_max_val_mcc_0.0_True_True_0_10',\n",
       " 'train_mcc': 0.3290569700460964,\n",
       " 'val_mcc': 0.3372137262553148,\n",
       " 'test_mcc': 0.42244825762846117,\n",
       " 'train_rmse': 0.9219544457292888,\n",
       " 'val_rmse': 0.9237604307034013,\n",
       " 'test_rmse': 0.8618916073713346,\n",
       " 'test_acc': 0.5419047474861145}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [5, 10, 15, 20],\n",
       " 'out_features': 4,\n",
       " 'activation': ReLU(),\n",
       " 'norm_edges': 'none',\n",
       " 'norm_nodes': 'gn',\n",
       " 'dropout': 0.0,\n",
       " 'train_lr': 0.1,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': True,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0.2,\n",
       " 'train_loss': 1.3397965,\n",
       " 'train_acc': 0.43666666746139526,\n",
       " 'val_acc': 0.5466666221618652,\n",
       " 'epoch': 21,\n",
       " 'model_class': 'gcn',\n",
       " 'path_name': '0.5466666221618652_4_[5_10_15_20]_4_ReLU()_none_gn_0.0_0.1_adamw_max_val_mcc_0.0_True_True_0.2',\n",
       " 'train_mcc': 0.2767504836717046,\n",
       " 'val_mcc': 0.41714835724534527,\n",
       " 'test_mcc': 0.34651784597969765,\n",
       " 'train_rmse': 1.055146119422961,\n",
       " 'val_rmse': 0.8679477710861024,\n",
       " 'test_rmse': 0.9616454152970859,\n",
       " 'test_acc': 0.4819047749042511}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_sage'\n",
    "log_dir = f'{log_path}_sage'\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = True,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "sage_model = dict(\n",
    "    in_features = [len(dataset.node_attributes)],\n",
    "    h_features = [[20] * 3, [25] * 3, [15] * 3, [10], [20,25,20], [30] * 3], \n",
    "    out_features = [out_feats],\n",
    "    aggregator_type = ['mean', 'lstm'],\n",
    "    norm_edges = ['right', 'none'],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    feat_drop = [0.2, 0],\n",
    "    # other\n",
    "    lr=[1e-2],\n",
    "    label_smoothing=[0.0, 0.2],\n",
    "    use_edge_weight=[True],\n",
    "    add_self_loop=[True],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(sage_model.keys(), k)) for k in itertools.product(*sage_model.values())]\n",
    "\n",
    "# sage_model = dict(\n",
    "#     in_features = [len(dataset.node_attributes)],\n",
    "#     h_features = [[25] * 3, [20] * 3], \n",
    "#     out_features = [out_feats],\n",
    "#     aggregator_type = ['lstm'],\n",
    "#     norm_edges = ['none'],\n",
    "#     norm_nodes = ['gn'],\n",
    "#     activation = [torch.nn.ReLU()],\n",
    "#     feat_drop = [0],\n",
    "#     # other\n",
    "#     lr=[1e-2],\n",
    "#     label_smoothing=[0.0],\n",
    "#     use_edge_weight=[True],\n",
    "#     add_self_loop=[True],\n",
    "#     drop_edges=[0],\n",
    "# )\n",
    "# list_model = [dict(zip(sage_model.keys(), k)) for k in itertools.product(*sage_model.values())]\n",
    "# list_model = [{i:j[k] for i,j in sage_model.items()} for k in range(len(sage_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [9:05:26<00:00, 56.82s/it]    \n"
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        ls = d.pop('label_smoothing')\n",
    "        add_self_loop = d.pop('add_self_loop')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        dataset_valid = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=0,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GraphSAGE(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_valid,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            label_smoothing=ls,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12596/12596 [44:11<00:00,  4.75it/s] \n"
     ]
    }
   ],
   "source": [
    "res_edges_sage = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [20, 25, 20],\n",
       " 'out_features': 4,\n",
       " 'aggregator_type': 'lstm',\n",
       " 'norm_edges': 'none',\n",
       " 'norm_nodes': 'bn',\n",
       " 'activation': ReLU(),\n",
       " 'feat_drop': 0,\n",
       " 'train_lr': 0.01,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': True,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 0.06970015,\n",
       " 'train_acc': 0.79666668176651,\n",
       " 'val_acc': 0.7933332324028015,\n",
       " 'epoch': 75,\n",
       " 'model_class': 'sage',\n",
       " 'path_name': '0.8066666126251221_4_[20_25_20]_4_lstm_none_bn_ReLU()_0_0.01_adamw_max_val_mcc_0.0_True_True_0',\n",
       " 'train_mcc': 0.7304961381714529,\n",
       " 'val_mcc': 0.7274500932572373,\n",
       " 'test_mcc': 0.7582125009246938,\n",
       " 'train_rmse': 0.49328828623162474,\n",
       " 'val_rmse': 0.565685424949238,\n",
       " 'test_rmse': 0.5246313898711125,\n",
       " 'test_acc': 0.8171428442001343}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_edges = res_edges_sage\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [20, 25, 20],\n",
       " 'out_features': 4,\n",
       " 'aggregator_type': 'lstm',\n",
       " 'norm_edges': 'none',\n",
       " 'norm_nodes': 'bn',\n",
       " 'activation': ReLU(),\n",
       " 'feat_drop': 0,\n",
       " 'train_lr': 0.01,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': True,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 0.06970015,\n",
       " 'train_acc': 0.79666668176651,\n",
       " 'val_acc': 0.7933332324028015,\n",
       " 'epoch': 75,\n",
       " 'model_class': 'sage',\n",
       " 'path_name': '0.8066666126251221_4_[20_25_20]_4_lstm_none_bn_ReLU()_0_0.01_adamw_max_val_mcc_0.0_True_True_0',\n",
       " 'train_mcc': 0.7304961381714529,\n",
       " 'val_mcc': 0.7274500932572373,\n",
       " 'test_mcc': 0.7582125009246938,\n",
       " 'train_rmse': 0.49328828623162474,\n",
       " 'val_rmse': 0.565685424949238,\n",
       " 'test_rmse': 0.5246313898711125,\n",
       " 'test_acc': 0.8171428442001343}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [30, 30, 30],\n",
       " 'out_features': 4,\n",
       " 'aggregator_type': 'lstm',\n",
       " 'norm_edges': 'none',\n",
       " 'norm_nodes': 'gn',\n",
       " 'activation': ReLU(),\n",
       " 'feat_drop': 0,\n",
       " 'train_lr': 0.01,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': True,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 0.022728005,\n",
       " 'train_acc': 0.8066666722297668,\n",
       " 'val_acc': 0.8933332562446594,\n",
       " 'epoch': 75,\n",
       " 'model_class': 'sage',\n",
       " 'path_name': '0.7866665720939636_4_[30_30_30]_4_lstm_none_gn_ReLU()_0_0.01_adamw_max_val_mcc_0.0_True_True_0',\n",
       " 'train_mcc': 0.7439325127527344,\n",
       " 'val_mcc': 0.8581177852228177,\n",
       " 'test_mcc': 0.7245683383176144,\n",
       " 'train_rmse': 0.4898979485566356,\n",
       " 'val_rmse': 0.408248290463863,\n",
       " 'test_rmse': 0.5145501965424799,\n",
       " 'test_acc': 0.7933333516120911}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_gat'\n",
    "log_dir = f'{log_path}_gat'\n",
    "\n",
    "add_self_loop = True\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = add_self_loop,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "gat_model = dict(\n",
    "    in_features = [len(dataset.node_attributes)],\n",
    "    h_features = [[10], [10] * 2, [15], [15] * 2, [20], [20] * 2, [25], [25] * 2],\n",
    "    out_features = [out_feats],\n",
    "    num_heads = [[4] * 2, [2] * 2, [4, 2]],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    negative_slope = [0.2, 0.4],\n",
    "    feat_drop = [0.2],\n",
    "    attn_drop = [0.2],\n",
    "    residual = [True],\n",
    "    # other\n",
    "    lr=[1e-2,],\n",
    "    label_smoothing=[0.0, 0.2],\n",
    "    use_edge_weight=[True],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(gat_model.keys(), k)) for k in itertools.product(*gat_model.values())]\n",
    "\n",
    "# gat_model = dict(\n",
    "#     in_features = [len(dataset.node_attributes)],\n",
    "#     # h_features = [[10], [15], [20]], \n",
    "#     h_features = [[10] * 3, [15] * 3, [20] * 3], \n",
    "#     out_features = [out_feats],\n",
    "#     # num_heads = [[4] * 4],\n",
    "#     num_heads = [[4, 2, 2]],\n",
    "#     norm_nodes = [None, 'bn', 'gn'],\n",
    "#     activation = [torch.nn.ReLU()],\n",
    "#     negative_slope = [0.2, 0.3, 0.4],\n",
    "#     feat_drop = [0.2],\n",
    "#     attn_drop = [0.2],\n",
    "#     residual = [True],\n",
    "#     # other\n",
    "#     lr=[1e-2,],\n",
    "#     label_smoothing=[0.0],\n",
    "#     use_edge_weight=[False],\n",
    "#     drop_edges=[0,],\n",
    "# )\n",
    "# list_model = [dict(zip(gat_model.keys(), k)) for k in itertools.product(*gat_model.values())]\n",
    "# list_model = [{i:j[k] for i,j in gat_model.items()} for k in range(len(gat_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [1:36:55<00:00,  5.05s/it]\n"
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        ls = d.pop('label_smoothing')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        # dataset_valid = ContagionDataset(\n",
    "        #     raw_dir=data_dir,\n",
    "        #     drop_edges=0,\n",
    "        #     sets_lengths=sets_lengths,\n",
    "        #     add_self_loop = add_self_loop,\n",
    "        #     target = target,\n",
    "        #     seed=seed,\n",
    "        # )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GAT(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            label_smoothing=ls,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26152/26152 [21:14<00:00, 20.52it/s]\n"
     ]
    }
   ],
   "source": [
    "res_edges_gat = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [20],\n",
       " 'out_features': 4,\n",
       " 'num_heads': [4, 2],\n",
       " 'norm_nodes': 'bn',\n",
       " 'activation': ReLU(),\n",
       " 'negative_slope': 0.2,\n",
       " 'feat_drop': 0.2,\n",
       " 'attn_drop': 0.2,\n",
       " 'residual': True,\n",
       " 'train_lr': 0.01,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.2,\n",
       " 'train_use_edge_weight': True,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 1.3458176,\n",
       " 'train_acc': 0.550000011920929,\n",
       " 'val_acc': 0.6333332657814026,\n",
       " 'epoch': 100,\n",
       " 'model_class': 'gat',\n",
       " 'path_name': '0.6333332657814026_4_[20]_4_[4_2]_bn_ReLU()_0.2_0.2_0.2_True_0.01_adamw_max_val_mcc_0.2_True_True_0_100',\n",
       " 'train_mcc': 0.4099457586398882,\n",
       " 'val_mcc': 0.511513513598694,\n",
       " 'test_mcc': 0.3999568821101237,\n",
       " 'train_rmse': 1.0630145812734648,\n",
       " 'val_rmse': 0.8679477710861024,\n",
       " 'test_rmse': 1.0151237316092017,\n",
       " 'test_acc': 0.5438095331192017}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_edges = res_edges_gat\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [20],\n",
       " 'out_features': 4,\n",
       " 'num_heads': [4, 2],\n",
       " 'norm_nodes': 'bn',\n",
       " 'activation': ReLU(),\n",
       " 'negative_slope': 0.2,\n",
       " 'feat_drop': 0.2,\n",
       " 'attn_drop': 0.2,\n",
       " 'residual': True,\n",
       " 'train_lr': 0.01,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.2,\n",
       " 'train_use_edge_weight': True,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 1.3458176,\n",
       " 'train_acc': 0.550000011920929,\n",
       " 'val_acc': 0.6333332657814026,\n",
       " 'epoch': 100,\n",
       " 'model_class': 'gat',\n",
       " 'path_name': '0.6333332657814026_4_[20]_4_[4_2]_bn_ReLU()_0.2_0.2_0.2_True_0.01_adamw_max_val_mcc_0.2_True_True_0_100',\n",
       " 'train_mcc': 0.4099457586398882,\n",
       " 'val_mcc': 0.511513513598694,\n",
       " 'test_mcc': 0.3999568821101237,\n",
       " 'train_rmse': 1.0630145812734648,\n",
       " 'val_rmse': 0.8679477710861024,\n",
       " 'test_rmse': 1.0151237316092017,\n",
       " 'test_acc': 0.5438095331192017}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_features': 4,\n",
       " 'h_features': [20],\n",
       " 'out_features': 4,\n",
       " 'num_heads': [4, 2],\n",
       " 'norm_nodes': 'gn',\n",
       " 'activation': ReLU(),\n",
       " 'negative_slope': 0.4,\n",
       " 'feat_drop': 0.2,\n",
       " 'attn_drop': 0.2,\n",
       " 'residual': True,\n",
       " 'train_lr': 0.01,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_scheduler_mode': 'max_val_mcc',\n",
       " 'train_label_smoothing': 0.0,\n",
       " 'train_use_edge_weight': False,\n",
       " 'train_self_loop': True,\n",
       " 'train_drop_edges': 0,\n",
       " 'train_loss': 1.1719619,\n",
       " 'train_acc': 0.5199999809265137,\n",
       " 'val_acc': 0.6466665863990784,\n",
       " 'epoch': 65,\n",
       " 'model_class': 'gat',\n",
       " 'path_name': '0.6466665863990784_4_[20]_4_[4_2]_gn_ReLU()_0.4_0.2_0.2_True_0.01_adamw_max_val_mcc_0.0_False_True_0',\n",
       " 'train_mcc': 0.3680872835097665,\n",
       " 'val_mcc': 0.5287608549600634,\n",
       " 'test_mcc': 0.3223399735489714,\n",
       " 'train_rmse': 1.1195237082497778,\n",
       " 'val_rmse': 0.8406346808612327,\n",
       " 'test_rmse': 1.1241928321286894,\n",
       " 'test_acc': 0.4828571379184723}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37be9487e307834247f9cc00a1ec46ceeb3f522b7edf17e3b2d74c6ce713e314"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
