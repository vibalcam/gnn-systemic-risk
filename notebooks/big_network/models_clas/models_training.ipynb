{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from models.train import train, test\n",
    "from models.models import GCN, GAT, GraphSAGE, FNN\n",
    "from models.utils import ContagionDataset, set_seed\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(ld, indent=0):\n",
    "    with open('result.txt', 'w', encoding='utf-8') as file:\n",
    "        for d in tqdm(ld):\n",
    "            file.write('{' + '\\n')\n",
    "            for key, value in d.items():\n",
    "                file.write('\\t' * (indent+1) + str(key) + ':' + str(value) + '\\n')\n",
    "                # file.write('\\t' * (indent+1) + str(key) + '\\n')\n",
    "                # file.write('\\t' * (indent+2) + str(value) + '\\n')\n",
    "            file.write('},\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = False\n",
    "\n",
    "seed = 4444\n",
    "set_seed(seed)\n",
    "\n",
    "metric_filter_1 = 'val_mcc'\n",
    "metric_filter_2 = 'test_mcc'\n",
    "\n",
    "data_dir = '../data'\n",
    "log_path = './logs'\n",
    "save_path = './saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big dataset: Additional stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_lengths = (0.5, 0.25, 0.25)\n",
    "target = 'additional_stress'\n",
    "\n",
    "dataset = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "out_feats = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(750)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset[0].ndata['train_mask']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(375)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset[0].ndata['val_mask']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(375)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset[0].ndata['test_mask']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_small_acc_train = {}\n",
    "dict_small_acc_val = {}\n",
    "dict_small_acc_test = {}\n",
    "dict_small_rmse_train = {}\n",
    "dict_small_rmse_val = {}\n",
    "dict_small_rmse_test = {}\n",
    "dict_small_mcc_train = {}\n",
    "dict_small_mcc_val = {}\n",
    "dict_small_mcc_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train,y_test = train_test_split(dataset.node_features[0].to_numpy(), dataset.targets[0], test_size=0.25, random_state=seed)\n",
    "g_data = dataset.graphs[0].ndata\n",
    "feats = g_data['feat']\n",
    "labels = g_data['label']\n",
    "train_mask = g_data['train_mask']\n",
    "val_mask = g_data['val_mask']\n",
    "test_mask = g_data['test_mask']\n",
    "\n",
    "# train + val for training, test for test\n",
    "x_train,x_test = feats[torch.logical_not(test_mask)], feats[test_mask]\n",
    "y_train,y_test = labels[torch.logical_not(test_mask)], labels[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1125, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([375, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.60      0.67        98\n",
      "           1       0.21      0.05      0.08        95\n",
      "           2       0.34      0.53      0.41        93\n",
      "           3       0.55      0.79      0.65        89\n",
      "\n",
      "    accuracy                           0.49       375\n",
      "   macro avg       0.46      0.49      0.45       375\n",
      "weighted avg       0.46      0.49      0.45       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial',random_state=seed, max_iter=800).fit(x_train, y_train)\n",
    "print(classification_report(y_true=y_test, y_pred=model_lr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.496\n",
      "Test accuracy: 0.488\n",
      "Train rmse: 0.9688481133111974\n",
      "Test rmse: 0.9605553948974868\n",
      "Train mcc: 0.34443142561242607\n",
      "Test mcc: 0.33524435059932634\n"
     ]
    }
   ],
   "source": [
    "dict_small_acc_train['logistic_regression'] = model_lr.score(x_train, y_train)\n",
    "dict_small_acc_test['logistic_regression'] = model_lr.score(x_test, y_test)\n",
    "print(f\"Train accuracy: {dict_small_acc_train['logistic_regression']}\")\n",
    "print(f\"Test accuracy: {dict_small_acc_test['logistic_regression']}\")\n",
    "\n",
    "dict_small_rmse_train['logistic_regression'] = mean_squared_error(y_true=y_train,y_pred=model_lr.predict(x_train), squared=False)\n",
    "dict_small_rmse_test['logistic_regression'] = mean_squared_error(y_true=y_test,y_pred=model_lr.predict(x_test), squared=False)\n",
    "print(f\"Train rmse: {dict_small_rmse_train['logistic_regression']}\")\n",
    "print(f\"Test rmse: {dict_small_rmse_test['logistic_regression']}\")\n",
    "\n",
    "dict_small_mcc_train['logistic_regression'] = matthews_corrcoef(y_true=y_train,y_pred=model_lr.predict(x_train))\n",
    "dict_small_mcc_test['logistic_regression'] = matthews_corrcoef(y_true=y_test,y_pred=model_lr.predict(x_test))\n",
    "print(f\"Train mcc: {dict_small_mcc_train['logistic_regression']}\")\n",
    "print(f\"Test mcc: {dict_small_mcc_test['logistic_regression']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_train_rf,x_val_rf,y_train_rf,y_val_rf = train_test_split(x_train, y_train, test_size=0.2, random_state=seed)\n",
    "# x_train_rf,x_val_rf,x_test_rf = feats[train_mask], feats[val_mask], feats[test_mask]\n",
    "# y_train_rf,y_val_rf,y_test_rf = labels[train_mask], labels[val_mask], labels[test_mask]\n",
    "x_train_rf = x_train\n",
    "y_train_rf = y_train\n",
    "x_val_rf = x_test\n",
    "y_val_rf = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3/21 [00:00<00:03,  5.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\notebooks\\big_network\\models_clas\\models_training.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/big_network/models_clas/models_training.ipynb#ch0000016?line=3'>4</a>\u001b[0m val_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/big_network/models_clas/models_training.ipynb#ch0000016?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m trange(\u001b[39m1\u001b[39m,num_nodes, (num_nodes \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/big_network/models_clas/models_training.ipynb#ch0000016?line=5'>6</a>\u001b[0m     tmp \u001b[39m=\u001b[39m RandomForestClassifier(random_state\u001b[39m=\u001b[39;49mseed, n_estimators\u001b[39m=\u001b[39;49mk)\u001b[39m.\u001b[39;49mfit(x_train_rf,y_train_rf)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/big_network/models_clas/models_training.ipynb#ch0000016?line=6'>7</a>\u001b[0m     tmp_acc \u001b[39m=\u001b[39m tmp\u001b[39m.\u001b[39mscore(x_val_rf, y_val_rf)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/big_network/models_clas/models_training.ipynb#ch0000016?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m val_acc \u001b[39m<\u001b[39m tmp_acc:\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=438'>439</a>\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=439'>440</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=440'>441</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=441'>442</a>\u001b[0m ]\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=443'>444</a>\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=444'>445</a>\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=445'>446</a>\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=446'>447</a>\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=447'>448</a>\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=448'>449</a>\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=449'>450</a>\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=450'>451</a>\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=451'>452</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=452'>453</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=453'>454</a>\u001b[0m )(\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=454'>455</a>\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=455'>456</a>\u001b[0m         t,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=456'>457</a>\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=457'>458</a>\u001b[0m         X,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=458'>459</a>\u001b[0m         y,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=459'>460</a>\u001b[0m         sample_weight,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=460'>461</a>\u001b[0m         i,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=461'>462</a>\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=462'>463</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=463'>464</a>\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=464'>465</a>\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=465'>466</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=466'>467</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=467'>468</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=469'>470</a>\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=470'>471</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=1042'>1043</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=1043'>1044</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=1045'>1046</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=1046'>1047</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=1049'>1050</a>\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=1050'>1051</a>\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=1051'>1052</a>\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=858'>859</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=859'>860</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=860'>861</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=861'>862</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=776'>777</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=777'>778</a>\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=778'>779</a>\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=779'>780</a>\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=780'>781</a>\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=781'>782</a>\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=782'>783</a>\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=783'>784</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=205'>206</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=206'>207</a>\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=207'>208</a>\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=208'>209</a>\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=209'>210</a>\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=568'>569</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=569'>570</a>\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=570'>571</a>\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/_parallel_backends.py?line=571'>572</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=257'>258</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=258'>259</a>\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=259'>260</a>\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=260'>261</a>\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=261'>262</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=262'>263</a>\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=257'>258</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=258'>259</a>\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=259'>260</a>\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=260'>261</a>\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=261'>262</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/joblib/parallel.py?line=262'>263</a>\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/utils/fixes.py?line=213'>214</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/utils/fixes.py?line=214'>215</a>\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/utils/fixes.py?line=215'>216</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=181'>182</a>\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=182'>183</a>\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=184'>185</a>\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=185'>186</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/ensemble/_forest.py?line=186'>187</a>\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=898'>899</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=899'>900</a>\u001b[0m     \u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, X_idx_sorted\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeprecated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=900'>901</a>\u001b[0m ):\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=901'>902</a>\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=902'>903</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=903'>904</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=933'>934</a>\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=934'>935</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=936'>937</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=937'>938</a>\u001b[0m         X,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=938'>939</a>\u001b[0m         y,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=939'>940</a>\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=940'>941</a>\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=941'>942</a>\u001b[0m         X_idx_sorted\u001b[39m=\u001b[39;49mX_idx_sorted,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=942'>943</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=943'>944</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\venv\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=408'>409</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=409'>410</a>\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=410'>411</a>\u001b[0m         splitter,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=411'>412</a>\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=416'>417</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=417'>418</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=419'>420</a>\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=421'>422</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/venv/lib/site-packages/sklearn/tree/_classes.py?line=422'>423</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "num_nodes = x_train_rf.shape[0]\n",
    "model_rf = None\n",
    "val_acc = 0.0\n",
    "for k in trange(1,num_nodes, (num_nodes - 1) // n):\n",
    "    tmp = RandomForestClassifier(random_state=seed, n_estimators=k).fit(x_train_rf,y_train_rf)\n",
    "    tmp_acc = tmp.score(x_val_rf, y_val_rf)\n",
    "    if val_acc < tmp_acc:\n",
    "        val_acc = tmp_acc\n",
    "        model_rf = tmp\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=model_rf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_small_acc_train['random_forest'] = model_rf.score(x_train_rf, y_train_rf)\n",
    "dict_small_acc_val['random_forest'] = model_rf.score(x_val_rf, y_val_rf)\n",
    "dict_small_acc_test['random_forest'] = model_rf.score(x_test, y_test)\n",
    "print(f\"Train accuracy: {dict_small_acc_train['random_forest']}\")\n",
    "print(f\"Val accuracy: {dict_small_acc_val['random_forest']}\")\n",
    "print(f\"Test accuracy: {dict_small_acc_test['random_forest']}\")\n",
    "\n",
    "dict_small_rmse_train['random_forest'] = mean_squared_error(y_true=y_train_rf,y_pred=model_rf.predict(x_train_rf), squared=False)\n",
    "dict_small_rmse_val['random_forest'] = mean_squared_error(y_true=y_val_rf,y_pred=model_rf.predict(x_val_rf), squared=False)\n",
    "dict_small_rmse_test['random_forest'] = mean_squared_error(y_true=y_test,y_pred=model_rf.predict(x_test), squared=False)\n",
    "print(f\"Train rmse: {dict_small_rmse_train['random_forest']}\")\n",
    "print(f\"Val rmse: {dict_small_rmse_val['random_forest']}\")\n",
    "print(f\"Test rmse: {dict_small_rmse_test['random_forest']}\")\n",
    "\n",
    "dict_small_mcc_train['random_forest'] = matthews_corrcoef(y_true=y_train_rf,y_pred=model_rf.predict(x_train_rf))\n",
    "dict_small_mcc_val['random_forest'] = matthews_corrcoef(y_true=y_val_rf,y_pred=model_rf.predict(x_val_rf))\n",
    "dict_small_mcc_test['random_forest'] = matthews_corrcoef(y_true=y_test,y_pred=model_rf.predict(x_test))\n",
    "print(f\"Train mcc: {dict_small_mcc_train['random_forest']}\")\n",
    "print(f\"Val mcc: {dict_small_mcc_val['random_forest']}\")\n",
    "print(f\"Test mcc: {dict_small_mcc_test['random_forest']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=out_feats).fit(x_train,y_train)\n",
    "print(classification_report(y_true=y_test, y_pred=model_knn.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_small_acc_train['knn_classifier'] = model_knn.score(x_train_rf, y_train_rf)\n",
    "dict_small_acc_test['knn_classifier'] = model_knn.score(x_test, y_test)\n",
    "print(f\"Train accuracy: {dict_small_acc_train['knn_classifier']}\")\n",
    "print(f\"Test accuracy: {dict_small_acc_test['knn_classifier']}\")\n",
    "\n",
    "dict_small_rmse_train['knn_classifier'] = mean_squared_error(y_true=y_train_rf,y_pred=model_knn.predict(x_train_rf), squared=False)\n",
    "dict_small_rmse_test['knn_classifier'] = mean_squared_error(y_true=y_test,y_pred=model_knn.predict(x_test), squared=False)\n",
    "print(f\"Train rmse: {dict_small_rmse_train['knn_classifier']}\")\n",
    "print(f\"Test rmse: {dict_small_rmse_test['knn_classifier']}\")\n",
    "\n",
    "dict_small_mcc_train['knn_classifier'] = matthews_corrcoef(y_true=y_train_rf,y_pred=model_knn.predict(x_train_rf))\n",
    "dict_small_mcc_test['knn_classifier'] = matthews_corrcoef(y_true=y_test,y_pred=model_knn.predict(x_test))\n",
    "print(f\"Train mcc: {dict_small_mcc_train['knn_classifier']}\")\n",
    "print(f\"Test mcc: {dict_small_mcc_test['knn_classifier']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_fnn'\n",
    "log_dir = f'{log_path}_fnn'\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = False,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "# fnn_model = dict(\n",
    "#     in_features=[len(dataset_val.node_attributes)],\n",
    "#     h_features=[[5, 10], [10, 15], [5,5,5], [5, 10, 15], [5, 10, 15, 20], [5], [10], [15]],\n",
    "#     out_features=[dataset_val.num_classes],\n",
    "#     activation=[torch.nn.ReLU()],\n",
    "#     norm_nodes = [None, 'bn', 'gn'],\n",
    "#     dropout=[0.2, 0.5, 0.0],\n",
    "#     # other\n",
    "#     lr=[1, 1e-1, 1e-2],\n",
    "#     label_smoothing=[0.0, 0.2, 0.4],\n",
    "# )\n",
    "\n",
    "fnn_model = dict(\n",
    "    in_features=[len(dataset_val.node_attributes)],\n",
    "    h_features=[[10, 15], [10, 15, 20], [5, 10, 15], [15] * 2, [15] * 3],\n",
    "    out_features=[dataset_val.num_classes],\n",
    "    activation=[torch.nn.ReLU()],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    dropout=[0.2, 0.0],\n",
    "    # other\n",
    "    lr=[1e-1],\n",
    "    label_smoothing=[0.0, 0.2],\n",
    ")\n",
    "list_model = [dict(zip(fnn_model.keys(), k)) for k in itertools.product(*fnn_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        ls = d.pop('label_smoothing')\n",
    "\n",
    "        train(\n",
    "            model=FNN(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_val,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            label_smoothing=ls,\n",
    "            use_edge_weight=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_fnn = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_fnn\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_small_acc_train)\n",
    "print(dict_small_acc_val)\n",
    "print(dict_small_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_small_mcc_train)\n",
    "print(dict_small_mcc_val)\n",
    "print(dict_small_mcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_small_rmse_train)\n",
    "print(dict_small_rmse_val)\n",
    "print(dict_small_rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_base = ConfusionMatrix(size=4)\n",
    "cm_base.add(preds=torch.as_tensor(model_rf.predict(x_test)), labels=torch.as_tensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_base.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_base.class_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm_base.labels==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_gcn'\n",
    "log_dir = f'{log_path}_gcn'\n",
    "\n",
    "add_self_loop = True\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = add_self_loop,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "gcn_model = dict(\n",
    "    in_features=[len(dataset_val.node_attributes)],\n",
    "    h_features=[[15] * 3, [10, 15, 20], [5, 10, 15, 20],],\n",
    "    out_features=[dataset_val.num_classes],\n",
    "    activation=[torch.nn.ReLU()],\n",
    "    norm_edges=['both', 'none'],\n",
    "    norm_nodes=[None, 'bn', 'gn'],\n",
    "    dropout=[0.2, 0.0],\n",
    "    # other\n",
    "    lr=[1e-1],\n",
    "    label_smoothing=[0.0, 0.2],\n",
    "    use_edge_weight=[True,],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(gcn_model.keys(), k)) for k in itertools.product(*gcn_model.values())]\n",
    "\n",
    "# gcn_model = dict(\n",
    "#     in_features=[len(dataset_val.node_attributes)],\n",
    "#     h_features=[[10] * 3],\n",
    "#     out_features=[dataset_val.num_classes],\n",
    "#     activation=[torch.nn.ReLU()],\n",
    "#     norm_edges=['both', 'none'],\n",
    "#     norm_nodes=[None, 'bn', 'gn'],\n",
    "#     dropout=[0.2, 0.0],\n",
    "#     # other\n",
    "#     lr=[1],\n",
    "#     label_smoothing=[0.0,],\n",
    "#     use_edge_weight=[True, ],\n",
    "#     drop_edges=[0,0.2],\n",
    "# )\n",
    "# list_model = [{i:j[k] for i,j in gcn_model.items()} for k in range(len(gcn_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        ls = d.pop('label_smoothing')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        # dataset_valid = ContagionDataset(\n",
    "        #     raw_dir=data_dir,\n",
    "        #     drop_edges=0,\n",
    "        #     sets_lengths=sets_lengths,\n",
    "        #     add_self_loop = add_self_loop,\n",
    "        #     target = target,\n",
    "        #     seed=seed,\n",
    "        # )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GCN(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            label_smoothing=ls,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_gcn = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_gcn\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_sage'\n",
    "log_dir = f'{log_path}_sage'\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = True,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "sage_model = dict(\n",
    "    in_features = [len(dataset.node_attributes)],\n",
    "    h_features = [[20] * 3, [25] * 3, [15] * 3, [10], [20,25,20], [30] * 3], \n",
    "    out_features = [out_feats],\n",
    "    # aggregator_type = ['mean', 'lstm'],\n",
    "    aggregator_type = ['lstm'],\n",
    "    norm_edges = ['right', 'none'],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    feat_drop = [0.2, 0],\n",
    "    # other\n",
    "    lr=[1e-2],\n",
    "    label_smoothing=[0.0, 0.2],\n",
    "    use_edge_weight=[True],\n",
    "    add_self_loop=[True],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(sage_model.keys(), k)) for k in itertools.product(*sage_model.values())]\n",
    "\n",
    "# sage_model = dict(\n",
    "#     in_features = [len(dataset.node_attributes)],\n",
    "#     h_features = [[30] * 3], \n",
    "#     out_features = [out_feats],\n",
    "#     aggregator_type = ['lstm'],\n",
    "#     norm_edges = ['none'],\n",
    "#     norm_nodes = ['gn'],\n",
    "#     activation = [torch.nn.ReLU()],\n",
    "#     feat_drop = [0],\n",
    "#     # other\n",
    "#     lr=[1e-2],\n",
    "#     label_smoothing=[0.0],\n",
    "#     use_edge_weight=[True],\n",
    "#     add_self_loop=[True],\n",
    "#     drop_edges=[0],\n",
    "# )\n",
    "# list_model = [{i:j[k] for i,j in sage_model.items()} for k in range(len(sage_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        ls = d.pop('label_smoothing')\n",
    "        add_self_loop = d.pop('add_self_loop')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        dataset_valid = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=0,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GraphSAGE(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_valid,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            label_smoothing=ls,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_sage = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_sage\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = all[sort_idx[0]]['test_cm'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm.labels==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.class_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_gat'\n",
    "log_dir = f'{log_path}_gat'\n",
    "\n",
    "add_self_loop = True\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = add_self_loop,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "gat_model = dict(\n",
    "    in_features = [len(dataset.node_attributes)],\n",
    "    h_features = [[10], [10] * 2, [15], [15] * 2, [20], [20] * 2, [25], [25] * 2],\n",
    "    out_features = [out_feats],\n",
    "    num_heads = [[4] * 2, [2] * 2, [4, 2]],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    negative_slope = [0.2, 0.4],\n",
    "    feat_drop = [0.2],\n",
    "    attn_drop = [0.2],\n",
    "    residual = [True],\n",
    "    # other\n",
    "    lr=[1e-2,],\n",
    "    label_smoothing=[0.0, 0.2],\n",
    "    use_edge_weight=[True],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(gat_model.keys(), k)) for k in itertools.product(*gat_model.values())]\n",
    "\n",
    "# gat_model = dict(\n",
    "#     in_features = [len(dataset.node_attributes)],\n",
    "#     # h_features = [[10], [15], [20]], \n",
    "#     h_features = [[10] * 3, [15] * 3, [20] * 3], \n",
    "#     out_features = [out_feats],\n",
    "#     # num_heads = [[4] * 4],\n",
    "#     num_heads = [[4, 2, 2]],\n",
    "#     norm_nodes = [None, 'bn', 'gn'],\n",
    "#     activation = [torch.nn.ReLU()],\n",
    "#     negative_slope = [0.2, 0.3, 0.4],\n",
    "#     feat_drop = [0.2],\n",
    "#     attn_drop = [0.2],\n",
    "#     residual = [True],\n",
    "#     # other\n",
    "#     lr=[1e-2,],\n",
    "#     label_smoothing=[0.0],\n",
    "#     use_edge_weight=[False],\n",
    "#     drop_edges=[0,],\n",
    "# )\n",
    "# list_model = [dict(zip(gat_model.keys(), k)) for k in itertools.product(*gat_model.values())]\n",
    "# list_model = [{i:j[k] for i,j in gat_model.items()} for k in range(len(gat_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        ls = d.pop('label_smoothing')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        # dataset_valid = ContagionDataset(\n",
    "        #     raw_dir=data_dir,\n",
    "        #     drop_edges=0,\n",
    "        #     sets_lengths=sets_lengths,\n",
    "        #     add_self_loop = add_self_loop,\n",
    "        #     target = target,\n",
    "        #     seed=seed,\n",
    "        # )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GAT(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            label_smoothing=ls,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_gat = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_gat\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37be9487e307834247f9cc00a1ec46ceeb3f522b7edf17e3b2d74c6ce713e314"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
