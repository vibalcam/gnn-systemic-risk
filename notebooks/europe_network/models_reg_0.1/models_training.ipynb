{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from win10toast import ToastNotifier\n",
    "toast = ToastNotifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from models.train_reg import train, test\n",
    "from models.models import GCN, GAT, GraphSAGE, FNN\n",
    "from models.utils import ContagionDataset, set_seed\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(ld, indent=0):\n",
    "    with open('result.txt', 'w', encoding='utf-8') as file:\n",
    "        for d in tqdm(ld):\n",
    "            file.write('{' + '\\n')\n",
    "            for key, value in d.items():\n",
    "                file.write('\\t' * (indent+1) + str(key) + ':' + str(value) + '\\n')\n",
    "                # file.write('\\t' * (indent+1) + str(key) + '\\n')\n",
    "                # file.write('\\t' * (indent+2) + str(value) + '\\n')\n",
    "            file.write('},\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = False\n",
    "\n",
    "seed = 4444\n",
    "set_seed(seed)\n",
    "\n",
    "metric_filter_1 = 'test_rmse_perc'\n",
    "metric_filter_2 = 'val_rmse_perc'\n",
    "\n",
    "data_dir = '../data'\n",
    "log_path = './logs'\n",
    "save_path = './saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big dataset: Additional stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_lengths = (0.07, 0.03, 0.9)\n",
    "target = 'additional_stress'\n",
    "\n",
    "dataset = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "out_feats = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_fnn'\n",
    "log_dir = f'{log_path}_fnn'\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = False,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "# fnn_model = dict(\n",
    "#     in_features=[len(dataset_val.node_attributes)],\n",
    "#     h_features=[[5, 10], [10, 15], [5,5,5], [5, 10, 15], [5, 10, 15, 20], [5], [10], [15]],\n",
    "#     out_features=[dataset_val.num_classes],\n",
    "#     activation=[torch.nn.ReLU()],\n",
    "#     norm_nodes = [None, 'bn', 'gn'],\n",
    "#     dropout=[0.2, 0.5, 0.0],\n",
    "#     # other\n",
    "#     lr=[1, 1e-1, 1e-2],\n",
    "#     label_smoothing=[0.0, 0.2, 0.4],\n",
    "# )\n",
    "\n",
    "fnn_model = dict(\n",
    "    in_features=[len(dataset_val.node_attributes)],\n",
    "    h_features=[[100], [100] * 2, [100] * 3, [200], [200]*2,[200]*3],\n",
    "    out_features=[dataset_val.num_classes],\n",
    "    activation=[torch.nn.ReLU()],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    dropout=[0.2, 0.0],\n",
    "    # other\n",
    "    lr=[1e-1],\n",
    ")\n",
    "list_model = [dict(zip(fnn_model.keys(), k)) for k in itertools.product(*fnn_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "\n",
    "        train(\n",
    "            model=FNN(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_val,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=200,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            use_edge_weight=False,\n",
    "            loss_type='mse',\n",
    "            scheduler_patience=20,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_fnn = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_fnn\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_gcn'\n",
    "log_dir = f'{log_path}_gcn'\n",
    "\n",
    "add_self_loop = True\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = add_self_loop,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "gcn_model = dict(\n",
    "    in_features=[len(dataset_val.node_attributes)],\n",
    "    h_features=[[15] * 3, [10, 15, 20], [5, 10, 15, 20],],\n",
    "    out_features=[dataset_val.num_classes],\n",
    "    activation=[torch.nn.ReLU()],\n",
    "    norm_edges=['both', 'none'],\n",
    "    norm_nodes=[None, 'bn', 'gn'],\n",
    "    dropout=[0.2, 0.0],\n",
    "    # other\n",
    "    lr=[1e-1],\n",
    "    use_edge_weight=[True,],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(gcn_model.keys(), k)) for k in itertools.product(*gcn_model.values())]\n",
    "\n",
    "# gcn_model = dict(\n",
    "#     in_features=[len(dataset_val.node_attributes)],\n",
    "#     h_features=[[10] * 3],\n",
    "#     out_features=[dataset_val.num_classes],\n",
    "#     activation=[torch.nn.ReLU()],\n",
    "#     norm_edges=['both', 'none'],\n",
    "#     norm_nodes=[None, 'bn', 'gn'],\n",
    "#     dropout=[0.2, 0.0],\n",
    "#     # other\n",
    "#     lr=[1],\n",
    "#     label_smoothing=[0.0,],\n",
    "#     use_edge_weight=[True, ],\n",
    "#     drop_edges=[0,0.2],\n",
    "# )\n",
    "# list_model = [{i:j[k] for i,j in gcn_model.items()} for k in range(len(gcn_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        # dataset_valid = ContagionDataset(\n",
    "        #     raw_dir=data_dir,\n",
    "        #     drop_edges=0,\n",
    "        #     sets_lengths=sets_lengths,\n",
    "        #     add_self_loop = add_self_loop,\n",
    "        #     target = target,\n",
    "        #     seed=seed,\n",
    "        # )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GCN(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "            loss_type='mse',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_gcn = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_gcn\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_sage'\n",
    "log_dir = f'{log_path}_sage'\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = True,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "# sage_model = dict(\n",
    "#     in_features = [len(dataset.node_attributes)],\n",
    "#     h_features = [[15] * 3, [20], [15], [20] * 2, [15] * 2, [25], [30]], \n",
    "#     out_features = [out_feats],\n",
    "#     aggregator_type = ['lstm'],\n",
    "#     norm_edges = ['right', 'none'],\n",
    "#     norm_nodes = [None, 'bn', 'gn'],\n",
    "#     activation = [torch.nn.ReLU()],\n",
    "#     feat_drop = [0.2, 0],\n",
    "#     # other\n",
    "#     lr=[1e-2],\n",
    "#     use_edge_weight=[True],\n",
    "#     add_self_loop=[True],\n",
    "#     drop_edges=[0,0.2],\n",
    "# )\n",
    "# list_model = [dict(zip(sage_model.keys(), k)) for k in itertools.product(*sage_model.values())]\n",
    "\n",
    "sage_model = dict(\n",
    "    in_features = [len(dataset.node_attributes)],\n",
    "    h_features=[[200]*2, [500]*2],\n",
    "    out_features = [out_feats],\n",
    "    aggregator_type = ['lstm'],\n",
    "    norm_edges = ['none'],\n",
    "    norm_nodes = ['gn'],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    feat_drop = [0.0],\n",
    "    # other\n",
    "    lr=[1e-2],\n",
    "    use_edge_weight=[True],\n",
    "    add_self_loop=[True],\n",
    "    drop_edges=[0],\n",
    ")\n",
    "list_model = [dict(zip(sage_model.keys(), k)) for k in itertools.product(*sage_model.values())]\n",
    "# list_model = [{i:j[k] for i,j in sage_model.items()} for k in range(len(sage_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        add_self_loop = d.pop('add_self_loop')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        dataset_valid = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=0,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GraphSAGE(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_valid,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=1000,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=100,\n",
    "            use_cpu=False,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "            scheduler_patience=500,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_sage = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    ")\n",
    "\n",
    "toast.show_toast(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_sage\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = all[sort_idx[0]]['test_cm'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm.labels==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.class_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mean_squared_error(y_true=cm.true_percentiles[cm.labels==k], y_pred=cm.pseudo_perc[cm.labels==k], squared=False) for k in range(0, cm.size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mean_absolute_error(y_true=cm.true_percentiles[cm.labels==k], y_pred=cm.pseudo_perc[cm.labels==k]) for k in range(0, cm.size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphSAGE Base_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_sage_base'\n",
    "log_dir = f'{log_path}_sage_base'\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = True,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "sage_model = dict(\n",
    "    in_features = [len(dataset.node_attributes)],\n",
    "    h_features = [[15] * 3, [20], [15], [20] * 2, [15] * 2, [25], [30]], \n",
    "    out_features = [out_feats],\n",
    "    aggregator_type = ['mean', 'lstm'],\n",
    "    norm_edges = ['right', 'none'],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    feat_drop = [0.2, 0],\n",
    "    # other\n",
    "    lr=[1e-2],\n",
    "    use_edge_weight=[True],\n",
    "    add_self_loop=[True],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(sage_model.keys(), k)) for k in itertools.product(*sage_model.values())]\n",
    "\n",
    "# sage_model = dict(\n",
    "#     in_features = [len(dataset.node_attributes)],\n",
    "#     h_features = [[30] * 3], \n",
    "#     out_features = [out_feats],\n",
    "#     aggregator_type = ['lstm'],\n",
    "#     norm_edges = ['none'],\n",
    "#     norm_nodes = ['gn'],\n",
    "#     activation = [torch.nn.ReLU()],\n",
    "#     feat_drop = [0],\n",
    "#     # other\n",
    "#     lr=[1e-2],\n",
    "#     label_smoothing=[0.0],\n",
    "#     use_edge_weight=[True],\n",
    "#     add_self_loop=[True],\n",
    "#     drop_edges=[0],\n",
    "# )\n",
    "# list_model = [{i:j[k] for i,j in sage_model.items()} for k in range(len(sage_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        add_self_loop = d.pop('add_self_loop')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        dataset_valid = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=0,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GraphSAGE(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_valid,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "            loss_type='mse',\n",
    "            base_n=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_sage_base = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=True,\n",
    "    base_n=True,\n",
    ")\n",
    "\n",
    "toast.show_toast(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_sage_base\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = all[sort_idx[0]]['test_cm'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm.labels==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.class_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mean_squared_error(y_true=cm.true_percentiles[cm.labels==k], y_pred=cm.pseudo_perc[cm.labels==k], squared=False) for k in range(0, cm.size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mean_absolute_error(y_true=cm.true_percentiles[cm.labels==k], y_pred=cm.pseudo_perc[cm.labels==k]) for k in range(0, cm.size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = f'{save_path}_gat'\n",
    "log_dir = f'{log_path}_gat'\n",
    "\n",
    "add_self_loop = True\n",
    "\n",
    "dataset_val = ContagionDataset(\n",
    "    raw_dir=data_dir,\n",
    "    drop_edges=0,\n",
    "    sets_lengths=sets_lengths,\n",
    "    add_self_loop = add_self_loop,\n",
    "    target = target,\n",
    ")\n",
    "\n",
    "gat_model = dict(\n",
    "    in_features = [len(dataset.node_attributes)],\n",
    "    h_features = [[10], [10] * 2, [15], [15] * 2, [20], [20] * 2, [25], [25] * 2],\n",
    "    out_features = [out_feats],\n",
    "    num_heads = [[4] * 2, [2] * 2, [4, 2]],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    negative_slope = [0.2, 0.4],\n",
    "    feat_drop = [0.2],\n",
    "    attn_drop = [0.2],\n",
    "    residual = [True],\n",
    "    # other\n",
    "    lr=[1e-2,],\n",
    "    use_edge_weight=[True, False],\n",
    "    drop_edges=[0,0.2],\n",
    ")\n",
    "list_model = [dict(zip(gat_model.keys(), k)) for k in itertools.product(*gat_model.values())]\n",
    "\n",
    "# gat_model = dict(\n",
    "#     in_features = [len(dataset.node_attributes)],\n",
    "#     # h_features = [[10], [15], [20]], \n",
    "#     h_features = [[10] * 3, [15] * 3, [20] * 3], \n",
    "#     out_features = [out_feats],\n",
    "#     # num_heads = [[4] * 4],\n",
    "#     num_heads = [[4, 2, 2]],\n",
    "#     norm_nodes = [None, 'bn', 'gn'],\n",
    "#     activation = [torch.nn.ReLU()],\n",
    "#     negative_slope = [0.2, 0.3, 0.4],\n",
    "#     feat_drop = [0.2],\n",
    "#     attn_drop = [0.2],\n",
    "#     residual = [True],\n",
    "#     # other\n",
    "#     lr=[1e-2,],\n",
    "#     label_smoothing=[0.0],\n",
    "#     use_edge_weight=[False],\n",
    "#     drop_edges=[0,],\n",
    "# )\n",
    "# list_model = [dict(zip(gat_model.keys(), k)) for k in itertools.product(*gat_model.values())]\n",
    "# list_model = [{i:j[k] for i,j in gat_model.items()} for k in range(len(gat_model['in_features']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        d = d.copy()\n",
    "        lr = d.pop('lr')\n",
    "        drop_edges = d.pop('drop_edges')\n",
    "        use_edge_weight = d.pop('use_edge_weight')\n",
    "\n",
    "        # dataset_valid = ContagionDataset(\n",
    "        #     raw_dir=data_dir,\n",
    "        #     drop_edges=0,\n",
    "        #     sets_lengths=sets_lengths,\n",
    "        #     add_self_loop = add_self_loop,\n",
    "        #     target = target,\n",
    "        #     seed=seed,\n",
    "        # )\n",
    "\n",
    "        dataset_train = ContagionDataset(\n",
    "            raw_dir=data_dir,\n",
    "            drop_edges=drop_edges,\n",
    "            sets_lengths=sets_lengths,\n",
    "            add_self_loop = add_self_loop,\n",
    "            target = target,\n",
    "        )\n",
    "\n",
    "        train(\n",
    "            model=GAT(**d),\n",
    "            dict_model=d,\n",
    "            dataset_train=dataset_train,\n",
    "            dataset_val=dataset_val,\n",
    "            log_dir=log_dir,\n",
    "            save_path=save_model,\n",
    "            lr=lr,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=100,\n",
    "            scheduler_mode='max_val_mcc',\n",
    "            debug_mode=False,\n",
    "            steps_save=10,\n",
    "            use_cpu=False,\n",
    "            use_edge_weight=use_edge_weight,\n",
    "            loss_type='mse',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges_gat = test(\n",
    "    dataset=dataset_val,\n",
    "    save_path=save_model,\n",
    "    n_runs=1,\n",
    "    debug_mode=False,\n",
    "    use_cpu=False,\n",
    "    save=True,\n",
    "    use_edge_weight=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_edges = res_edges_gat\n",
    "res_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_edges[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty([all[k]['dict'] for k in sort_idx])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37be9487e307834247f9cc00a1ec46ceeb3f522b7edf17e3b2d74c6ce713e314"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
