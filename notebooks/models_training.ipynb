{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from models.train import train\n",
    "from models.models import GCN, GAT, GraphSAGE\n",
    "from models.utils import ContagionDataset\n",
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = ContagionDataset(\n",
    "    raw_dir='../data',\n",
    "    drop_edges=0,\n",
    "    sets_lengths=(0.8,0.1,0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model = dict(\n",
    "    in_features = [dataset.node_features],\n",
    "    h_features = [[5,5], [5,10],[10,5]],\n",
    "    out_features = [dataset.num_classes],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    norm_edges = ['both'],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    dropout = [0.2],\n",
    "    # other\n",
    "    lr = [1e-2],\n",
    ")\n",
    "list_gcn_model = [dict(zip(gcn_model.keys(), k)) for k in itertools.product(*gcn_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_model = dict(\n",
    "    in_features = [dataset.node_features],\n",
    "    h_features = [[5,5], [5,10],[10,5]], \n",
    "    out_features = [dataset.num_classes],\n",
    "    aggregator_type = ['mean', 'gcn', 'pool', 'lstm'],\n",
    "    norm_edges = ['both', 'none'],\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    activation = [torch.nn.ReLU()],\n",
    "    feat_drop = [0.2],\n",
    "    # other\n",
    "    lr = [1e-2],\n",
    ")\n",
    "list_sage_model = [dict(zip(sage_model.keys(), k)) for k in itertools.product(*sage_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_model = dict(\n",
    "    in_features = [dataset.node_features],\n",
    "    h_features = [[5,5], [5,10],[10,5]],\n",
    "    out_features = [dataset.num_classes],\n",
    "    num_heads = [[dataset.node_features] * 2] * 3,\n",
    "    norm_nodes = [None, 'bn', 'gn'],\n",
    "    activation = [None],\n",
    "    negative_slope = [0.2],\n",
    "    feat_drop = [0.2],\n",
    "    attn_drop = [0.2],\n",
    "    residual = [False, True],\n",
    "    # other\n",
    "    lr = [0.005],\n",
    ")\n",
    "list_gat_model = [dict(zip(gat_model.keys(), k)) for k in itertools.product(*gat_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "in_features=2/h_features=[5/5]/out_features=4/activation=ReLU()/norm_edges=both/norm_nodes=None/dropout=0.2/lr=0.01///lr=0.01/optimizer_name=adamw/scheduler_mode=max_val_acc/label_smoothing=0.0/use_edge_weight=True\n",
      "0 of 200\n",
      "Best val acc 0: 0.24999980628490448\n",
      "1 of 200\n",
      "Best val acc 1: 0.24999980628490448\n",
      "2 of 200\n",
      "3 of 200\n",
      "4 of 200\n",
      "5 of 200\n",
      "6 of 200\n",
      "Best val acc 6: 0.24999980628490448\n",
      "7 of 200\n",
      "Best val acc 7: 0.24999980628490448\n",
      "8 of 200\n",
      "Best val acc 8: 0.24999980628490448\n",
      "9 of 200\n",
      "Best val acc 9: 0.3333330750465393\n",
      "10 of 200\n",
      "Best val acc 10: 0.3333330750465393\n",
      "11 of 200\n",
      "Best val acc 11: 0.3333330750465393\n",
      "12 of 200\n",
      "Best val acc 12: 0.3333330750465393\n",
      "13 of 200\n",
      "Best val acc 13: 0.3333330750465393\n",
      "14 of 200\n",
      "Best val acc 14: 0.3333330750465393\n",
      "15 of 200\n",
      "Best val acc 15: 0.3333330750465393\n",
      "16 of 200\n",
      "Best val acc 16: 0.3333330750465393\n",
      "17 of 200\n",
      "Best val acc 17: 0.3333330750465393\n",
      "18 of 200\n",
      "Best val acc 18: 0.41666632890701294\n",
      "19 of 200\n",
      "Best val acc 19: 0.583332896232605\n",
      "20 of 200\n",
      "Best val acc 20: 0.583332896232605\n",
      "21 of 200\n",
      "Best val acc 21: 0.583332896232605\n",
      "22 of 200\n",
      "Best val acc 22: 0.583332896232605\n",
      "23 of 200\n",
      "Best val acc 23: 0.583332896232605\n",
      "24 of 200\n",
      "Best val acc 24: 0.583332896232605\n",
      "25 of 200\n",
      "Best val acc 25: 0.583332896232605\n",
      "26 of 200\n",
      "Best val acc 26: 0.583332896232605\n",
      "27 of 200\n",
      "Best val acc 27: 0.583332896232605\n",
      "28 of 200\n",
      "Best val acc 28: 0.583332896232605\n",
      "29 of 200\n",
      "Best val acc 29: 0.583332896232605\n",
      "30 of 200\n",
      "Best val acc 30: 0.583332896232605\n",
      "31 of 200\n",
      "Best val acc 31: 0.583332896232605\n",
      "32 of 200\n",
      "Best val acc 32: 0.583332896232605\n",
      "33 of 200\n",
      "Best val acc 33: 0.583332896232605\n",
      "34 of 200\n",
      "Best val acc 34: 0.583332896232605\n",
      "35 of 200\n",
      "Best val acc 35: 0.6666661500930786\n",
      "36 of 200\n",
      "Best val acc 36: 0.6666661500930786\n",
      "37 of 200\n",
      "Best val acc 37: 0.6666661500930786\n",
      "38 of 200\n",
      "Best val acc 38: 0.6666661500930786\n",
      "39 of 200\n",
      "Best val acc 39: 0.6666661500930786\n",
      "40 of 200\n",
      "Best val acc 40: 0.6666661500930786\n",
      "41 of 200\n",
      "42 of 200\n",
      "43 of 200\n",
      "Best val acc 43: 0.6666661500930786\n",
      "44 of 200\n",
      "Best val acc 44: 0.6666661500930786\n",
      "45 of 200\n",
      "Best val acc 45: 0.6666661500930786\n",
      "46 of 200\n",
      "Best val acc 46: 0.6666661500930786\n",
      "47 of 200\n",
      "Best val acc 47: 0.6666661500930786\n",
      "48 of 200\n",
      "Best val acc 48: 0.6666661500930786\n",
      "49 of 200\n",
      "Best val acc 49: 0.6666661500930786\n",
      "50 of 200\n",
      "Best val acc 50: 0.6666661500930786\n",
      "51 of 200\n",
      "Best val acc 51: 0.6666661500930786\n",
      "52 of 200\n",
      "Best val acc 52: 0.6666661500930786\n",
      "53 of 200\n",
      "Best val acc 53: 0.6666661500930786\n",
      "54 of 200\n",
      "Best val acc 54: 0.6666661500930786\n",
      "55 of 200\n",
      "Best val acc 55: 0.6666661500930786\n",
      "56 of 200\n",
      "Best val acc 56: 0.6666661500930786\n",
      "57 of 200\n",
      "Best val acc 57: 0.6666661500930786\n",
      "58 of 200\n",
      "Best val acc 58: 0.6666661500930786\n",
      "59 of 200\n",
      "Best val acc 59: 0.6666661500930786\n",
      "60 of 200\n",
      "Best val acc 60: 0.6666661500930786\n",
      "61 of 200\n",
      "Best val acc 61: 0.6666661500930786\n",
      "62 of 200\n",
      "Best val acc 62: 0.6666661500930786\n",
      "63 of 200\n",
      "Best val acc 63: 0.6666661500930786\n",
      "64 of 200\n",
      "Best val acc 64: 0.6666661500930786\n",
      "65 of 200\n",
      "Best val acc 65: 0.6666661500930786\n",
      "66 of 200\n",
      "Best val acc 66: 0.6666661500930786\n",
      "67 of 200\n",
      "Best val acc 67: 0.6666661500930786\n",
      "68 of 200\n",
      "Best val acc 68: 0.6666661500930786\n",
      "69 of 200\n",
      "Best val acc 69: 0.6666661500930786\n",
      "70 of 200\n",
      "Best val acc 70: 0.6666661500930786\n",
      "71 of 200\n",
      "Best val acc 71: 0.6666661500930786\n",
      "72 of 200\n",
      "Best val acc 72: 0.6666661500930786\n",
      "73 of 200\n",
      "Best val acc 73: 0.6666661500930786\n",
      "74 of 200\n",
      "Best val acc 74: 0.6666661500930786\n",
      "75 of 200\n",
      "Best val acc 75: 0.6666661500930786\n",
      "76 of 200\n",
      "Best val acc 76: 0.6666661500930786\n",
      "77 of 200\n",
      "Best val acc 77: 0.6666661500930786\n",
      "78 of 200\n",
      "Best val acc 78: 0.6666661500930786\n",
      "79 of 200\n",
      "Best val acc 79: 0.6666661500930786\n",
      "80 of 200\n",
      "Best val acc 80: 0.6666661500930786\n",
      "81 of 200\n",
      "Best val acc 81: 0.6666661500930786\n",
      "82 of 200\n",
      "Best val acc 82: 0.6666661500930786\n",
      "83 of 200\n",
      "Best val acc 83: 0.6666661500930786\n",
      "84 of 200\n",
      "Best val acc 84: 0.6666661500930786\n",
      "85 of 200\n",
      "Best val acc 85: 0.6666661500930786\n",
      "86 of 200\n",
      "Best val acc 86: 0.6666661500930786\n",
      "87 of 200\n",
      "Best val acc 87: 0.6666661500930786\n",
      "88 of 200\n",
      "Best val acc 88: 0.6666661500930786\n",
      "89 of 200\n",
      "Best val acc 89: 0.6666661500930786\n",
      "90 of 200\n",
      "Best val acc 90: 0.6666661500930786\n",
      "91 of 200\n",
      "Best val acc 91: 0.6666661500930786\n",
      "92 of 200\n",
      "Best val acc 92: 0.6666661500930786\n",
      "93 of 200\n",
      "Best val acc 93: 0.6666661500930786\n",
      "94 of 200\n",
      "Best val acc 94: 0.6666661500930786\n",
      "95 of 200\n",
      "Best val acc 95: 0.6666661500930786\n",
      "96 of 200\n",
      "Best val acc 96: 0.6666661500930786\n",
      "97 of 200\n",
      "Best val acc 97: 0.6666661500930786\n",
      "98 of 200\n",
      "Best val acc 98: 0.6666661500930786\n",
      "99 of 200\n",
      "Best val acc 99: 0.6666661500930786\n",
      "100 of 200\n",
      "Best val acc 100: 0.6666661500930786\n",
      "101 of 200\n",
      "Best val acc 101: 0.6666661500930786\n",
      "102 of 200\n",
      "Best val acc 102: 0.6666661500930786\n",
      "103 of 200\n",
      "Best val acc 103: 0.6666661500930786\n",
      "104 of 200\n",
      "Best val acc 104: 0.6666661500930786\n",
      "105 of 200\n",
      "Best val acc 105: 0.6666661500930786\n",
      "106 of 200\n",
      "Best val acc 106: 0.6666661500930786\n",
      "107 of 200\n",
      "Best val acc 107: 0.6666661500930786\n",
      "108 of 200\n",
      "Best val acc 108: 0.6666661500930786\n",
      "109 of 200\n",
      "Best val acc 109: 0.6666661500930786\n",
      "110 of 200\n",
      "Best val acc 110: 0.6666661500930786\n",
      "111 of 200\n",
      "Best val acc 111: 0.6666661500930786\n",
      "112 of 200\n",
      "Best val acc 112: 0.6666661500930786\n",
      "113 of 200\n",
      "Best val acc 113: 0.6666661500930786\n",
      "114 of 200\n",
      "Best val acc 114: 0.6666661500930786\n",
      "115 of 200\n",
      "Best val acc 115: 0.6666661500930786\n",
      "116 of 200\n",
      "Best val acc 116: 0.6666661500930786\n",
      "117 of 200\n",
      "Best val acc 117: 0.6666661500930786\n",
      "118 of 200\n",
      "Best val acc 118: 0.6666661500930786\n",
      "119 of 200\n",
      "Best val acc 119: 0.6666661500930786\n",
      "120 of 200\n",
      "Best val acc 120: 0.6666661500930786\n",
      "121 of 200\n",
      "Best val acc 121: 0.6666661500930786\n",
      "122 of 200\n",
      "Best val acc 122: 0.6666661500930786\n",
      "123 of 200\n",
      "Best val acc 123: 0.6666661500930786\n",
      "124 of 200\n",
      "Best val acc 124: 0.6666661500930786\n",
      "125 of 200\n",
      "Best val acc 125: 0.6666661500930786\n",
      "126 of 200\n",
      "Best val acc 126: 0.6666661500930786\n",
      "127 of 200\n",
      "Best val acc 127: 0.6666661500930786\n",
      "128 of 200\n",
      "Best val acc 128: 0.6666661500930786\n",
      "129 of 200\n",
      "Best val acc 129: 0.6666661500930786\n",
      "130 of 200\n",
      "Best val acc 130: 0.6666661500930786\n",
      "131 of 200\n",
      "Best val acc 131: 0.6666661500930786\n",
      "132 of 200\n",
      "Best val acc 132: 0.6666661500930786\n",
      "133 of 200\n",
      "Best val acc 133: 0.6666661500930786\n",
      "134 of 200\n",
      "Best val acc 134: 0.6666661500930786\n",
      "135 of 200\n",
      "Best val acc 135: 0.6666661500930786\n",
      "136 of 200\n",
      "Best val acc 136: 0.6666661500930786\n",
      "137 of 200\n",
      "Best val acc 137: 0.6666661500930786\n",
      "138 of 200\n",
      "Best val acc 138: 0.6666661500930786\n",
      "139 of 200\n",
      "Best val acc 139: 0.6666661500930786\n",
      "140 of 200\n",
      "Best val acc 140: 0.6666661500930786\n",
      "141 of 200\n",
      "Best val acc 141: 0.6666661500930786\n",
      "142 of 200\n",
      "Best val acc 142: 0.6666661500930786\n",
      "143 of 200\n",
      "Best val acc 143: 0.6666661500930786\n",
      "144 of 200\n",
      "Best val acc 144: 0.6666661500930786\n",
      "145 of 200\n",
      "Best val acc 145: 0.6666661500930786\n",
      "146 of 200\n",
      "Best val acc 146: 0.6666661500930786\n",
      "147 of 200\n",
      "Best val acc 147: 0.6666661500930786\n",
      "148 of 200\n",
      "Best val acc 148: 0.6666661500930786\n",
      "149 of 200\n",
      "Best val acc 149: 0.6666661500930786\n",
      "150 of 200\n",
      "Best val acc 150: 0.6666661500930786\n",
      "151 of 200\n",
      "Best val acc 151: 0.6666661500930786\n",
      "152 of 200\n",
      "Best val acc 152: 0.6666661500930786\n",
      "153 of 200\n",
      "Best val acc 153: 0.6666661500930786\n",
      "154 of 200\n",
      "Best val acc 154: 0.6666661500930786\n",
      "155 of 200\n",
      "Best val acc 155: 0.6666661500930786\n",
      "156 of 200\n",
      "Best val acc 156: 0.6666661500930786\n",
      "157 of 200\n",
      "Best val acc 157: 0.6666661500930786\n",
      "158 of 200\n",
      "Best val acc 158: 0.6666661500930786\n",
      "159 of 200\n",
      "Best val acc 159: 0.6666661500930786\n",
      "160 of 200\n",
      "Best val acc 160: 0.6666661500930786\n",
      "161 of 200\n",
      "Best val acc 161: 0.6666661500930786\n",
      "162 of 200\n",
      "Best val acc 162: 0.6666661500930786\n",
      "163 of 200\n",
      "Best val acc 163: 0.6666661500930786\n",
      "164 of 200\n",
      "Best val acc 164: 0.6666661500930786\n",
      "165 of 200\n",
      "Best val acc 165: 0.6666661500930786\n",
      "166 of 200\n",
      "Best val acc 166: 0.6666661500930786\n",
      "167 of 200\n",
      "Best val acc 167: 0.6666661500930786\n",
      "168 of 200\n",
      "Best val acc 168: 0.6666661500930786\n",
      "169 of 200\n",
      "Best val acc 169: 0.6666661500930786\n",
      "170 of 200\n",
      "Best val acc 170: 0.6666661500930786\n",
      "171 of 200\n",
      "Best val acc 171: 0.6666661500930786\n",
      "172 of 200\n",
      "Best val acc 172: 0.6666661500930786\n",
      "173 of 200\n",
      "Best val acc 173: 0.6666661500930786\n",
      "174 of 200\n",
      "Best val acc 174: 0.6666661500930786\n",
      "175 of 200\n",
      "Best val acc 175: 0.6666661500930786\n",
      "176 of 200\n",
      "Best val acc 176: 0.6666661500930786\n",
      "177 of 200\n",
      "Best val acc 177: 0.6666661500930786\n",
      "178 of 200\n",
      "Best val acc 178: 0.6666661500930786\n",
      "179 of 200\n",
      "Best val acc 179: 0.6666661500930786\n",
      "180 of 200\n",
      "Best val acc 180: 0.6666661500930786\n",
      "181 of 200\n",
      "Best val acc 181: 0.6666661500930786\n",
      "182 of 200\n",
      "Best val acc 182: 0.6666661500930786\n",
      "183 of 200\n",
      "Best val acc 183: 0.6666661500930786\n",
      "184 of 200\n",
      "Best val acc 184: 0.6666661500930786\n",
      "185 of 200\n",
      "Best val acc 185: 0.6666661500930786\n",
      "186 of 200\n",
      "Best val acc 186: 0.6666661500930786\n",
      "187 of 200\n",
      "Best val acc 187: 0.6666661500930786\n",
      "188 of 200\n",
      "Best val acc 188: 0.6666661500930786\n",
      "189 of 200\n",
      "Best val acc 189: 0.6666661500930786\n",
      "190 of 200\n",
      "Best val acc 190: 0.6666661500930786\n",
      "191 of 200\n",
      "Best val acc 191: 0.6666661500930786\n",
      "192 of 200\n",
      "Best val acc 192: 0.6666661500930786\n",
      "193 of 200\n",
      "Best val acc 193: 0.6666661500930786\n",
      "194 of 200\n",
      "Best val acc 194: 0.6666661500930786\n",
      "195 of 200\n",
      "Best val acc 195: 0.6666661500930786\n",
      "196 of 200\n",
      "Best val acc 196: 0.6666661500930786\n",
      "197 of 200\n",
      "Best val acc 197: 0.6666661500930786\n",
      "198 of 200\n",
      "Best val acc 198: 0.6666661500930786\n",
      "199 of 200\n",
      "Best val acc 199: 0.6666661500930786\n",
      "cuda\n",
      "in_features=2/h_features=[5/5]/out_features=4/activation=ReLU()/norm_edges=both/norm_nodes=bn/dropout=0.2/lr=0.01///lr=0.01/optimizer_name=adamw/scheduler_mode=max_val_acc/label_smoothing=0.0/use_edge_weight=True\n",
      "0 of 200\n",
      "Best val acc 0: 0.24999980628490448\n",
      "1 of 200\n",
      "2 of 200\n",
      "3 of 200\n",
      "Best val acc 3: 0.24999980628490448\n",
      "4 of 200\n",
      "Best val acc 4: 0.24999980628490448\n",
      "5 of 200\n",
      "Best val acc 5: 0.24999980628490448\n",
      "6 of 200\n",
      "Best val acc 6: 0.24999980628490448\n",
      "7 of 200\n",
      "Best val acc 7: 0.24999980628490448\n",
      "8 of 200\n",
      "Best val acc 8: 0.24999980628490448\n",
      "9 of 200\n",
      "Best val acc 9: 0.24999980628490448\n",
      "10 of 200\n",
      "Best val acc 10: 0.3333330750465393\n",
      "11 of 200\n",
      "Best val acc 11: 0.3333330750465393\n",
      "12 of 200\n",
      "Best val acc 12: 0.3333330750465393\n",
      "13 of 200\n",
      "Best val acc 13: 0.3333330750465393\n",
      "14 of 200\n",
      "Best val acc 14: 0.41666632890701294\n",
      "15 of 200\n",
      "Best val acc 15: 0.41666632890701294\n",
      "16 of 200\n",
      "Best val acc 16: 0.41666632890701294\n",
      "17 of 200\n",
      "Best val acc 17: 0.41666632890701294\n",
      "18 of 200\n",
      "Best val acc 18: 0.41666632890701294\n",
      "19 of 200\n",
      "Best val acc 19: 0.41666632890701294\n",
      "20 of 200\n",
      "Best val acc 20: 0.41666632890701294\n",
      "21 of 200\n",
      "Best val acc 21: 0.41666632890701294\n",
      "22 of 200\n",
      "Best val acc 22: 0.41666632890701294\n",
      "23 of 200\n",
      "Best val acc 23: 0.41666632890701294\n",
      "24 of 200\n",
      "Best val acc 24: 0.41666632890701294\n",
      "25 of 200\n",
      "Best val acc 25: 0.41666632890701294\n",
      "26 of 200\n",
      "Best val acc 26: 0.41666632890701294\n",
      "27 of 200\n",
      "Best val acc 27: 0.41666632890701294\n",
      "28 of 200\n",
      "Best val acc 28: 0.41666632890701294\n",
      "29 of 200\n",
      "Best val acc 29: 0.41666632890701294\n",
      "30 of 200\n",
      "Best val acc 30: 0.41666632890701294\n",
      "31 of 200\n",
      "Best val acc 31: 0.41666632890701294\n",
      "32 of 200\n",
      "Best val acc 32: 0.41666632890701294\n",
      "33 of 200\n",
      "Best val acc 33: 0.41666632890701294\n",
      "34 of 200\n",
      "Best val acc 34: 0.41666632890701294\n",
      "35 of 200\n",
      "Best val acc 35: 0.41666632890701294\n",
      "36 of 200\n",
      "Best val acc 36: 0.41666632890701294\n",
      "37 of 200\n",
      "Best val acc 37: 0.41666632890701294\n",
      "38 of 200\n",
      "Best val acc 38: 0.41666632890701294\n",
      "39 of 200\n",
      "Best val acc 39: 0.41666632890701294\n",
      "40 of 200\n",
      "Best val acc 40: 0.41666632890701294\n",
      "41 of 200\n",
      "Best val acc 41: 0.41666632890701294\n",
      "42 of 200\n",
      "Best val acc 42: 0.41666632890701294\n",
      "43 of 200\n",
      "Best val acc 43: 0.41666632890701294\n",
      "44 of 200\n",
      "Best val acc 44: 0.41666632890701294\n",
      "45 of 200\n",
      "Best val acc 45: 0.41666632890701294\n",
      "46 of 200\n",
      "Best val acc 46: 0.41666632890701294\n",
      "47 of 200\n",
      "Best val acc 47: 0.41666632890701294\n",
      "48 of 200\n",
      "Best val acc 48: 0.41666632890701294\n",
      "49 of 200\n",
      "Best val acc 49: 0.41666632890701294\n",
      "50 of 200\n",
      "Best val acc 50: 0.41666632890701294\n",
      "51 of 200\n",
      "Best val acc 51: 0.41666632890701294\n",
      "52 of 200\n",
      "Best val acc 52: 0.41666632890701294\n",
      "53 of 200\n",
      "Best val acc 53: 0.41666632890701294\n",
      "54 of 200\n",
      "Best val acc 54: 0.41666632890701294\n",
      "55 of 200\n",
      "Best val acc 55: 0.41666632890701294\n",
      "56 of 200\n",
      "Best val acc 56: 0.41666632890701294\n",
      "57 of 200\n",
      "Best val acc 57: 0.41666632890701294\n",
      "58 of 200\n",
      "Best val acc 58: 0.41666632890701294\n",
      "59 of 200\n",
      "Best val acc 59: 0.41666632890701294\n",
      "60 of 200\n",
      "Best val acc 60: 0.41666632890701294\n",
      "61 of 200\n",
      "Best val acc 61: 0.41666632890701294\n",
      "62 of 200\n",
      "Best val acc 62: 0.41666632890701294\n",
      "63 of 200\n",
      "Best val acc 63: 0.41666632890701294\n",
      "64 of 200\n",
      "Best val acc 64: 0.49999961256980896\n",
      "65 of 200\n",
      "Best val acc 65: 0.49999961256980896\n",
      "66 of 200\n",
      "Best val acc 66: 0.49999961256980896\n",
      "67 of 200\n",
      "68 of 200\n",
      "69 of 200\n",
      "70 of 200\n",
      "71 of 200\n",
      "72 of 200\n",
      "73 of 200\n",
      "74 of 200\n",
      "75 of 200\n",
      "76 of 200\n",
      "Best val acc 76: 0.49999961256980896\n",
      "77 of 200\n",
      "Best val acc 77: 0.49999961256980896\n",
      "78 of 200\n",
      "Best val acc 78: 0.49999961256980896\n",
      "79 of 200\n",
      "Best val acc 79: 0.49999961256980896\n",
      "80 of 200\n",
      "Best val acc 80: 0.49999961256980896\n",
      "81 of 200\n",
      "Best val acc 81: 0.49999961256980896\n",
      "82 of 200\n",
      "Best val acc 82: 0.49999961256980896\n",
      "83 of 200\n",
      "Best val acc 83: 0.49999961256980896\n",
      "84 of 200\n",
      "Best val acc 84: 0.49999961256980896\n",
      "85 of 200\n",
      "Best val acc 85: 0.49999961256980896\n",
      "86 of 200\n",
      "Best val acc 86: 0.49999961256980896\n",
      "87 of 200\n",
      "Best val acc 87: 0.49999961256980896\n",
      "88 of 200\n",
      "Best val acc 88: 0.49999961256980896\n",
      "89 of 200\n",
      "Best val acc 89: 0.49999961256980896\n",
      "90 of 200\n",
      "Best val acc 90: 0.49999961256980896\n",
      "91 of 200\n",
      "Best val acc 91: 0.49999961256980896\n",
      "92 of 200\n",
      "Best val acc 92: 0.49999961256980896\n",
      "93 of 200\n",
      "Best val acc 93: 0.49999961256980896\n",
      "94 of 200\n",
      "Best val acc 94: 0.49999961256980896\n",
      "95 of 200\n",
      "Best val acc 95: 0.49999961256980896\n",
      "96 of 200\n",
      "Best val acc 96: 0.49999961256980896\n",
      "97 of 200\n",
      "Best val acc 97: 0.49999961256980896\n",
      "98 of 200\n",
      "Best val acc 98: 0.49999961256980896\n",
      "99 of 200\n",
      "Best val acc 99: 0.49999961256980896\n",
      "100 of 200\n",
      "Best val acc 100: 0.49999961256980896\n",
      "101 of 200\n",
      "Best val acc 101: 0.49999961256980896\n",
      "102 of 200\n",
      "Best val acc 102: 0.49999961256980896\n",
      "103 of 200\n",
      "Best val acc 103: 0.49999961256980896\n",
      "104 of 200\n",
      "Best val acc 104: 0.49999961256980896\n",
      "105 of 200\n",
      "Best val acc 105: 0.49999961256980896\n",
      "106 of 200\n",
      "Best val acc 106: 0.49999961256980896\n",
      "107 of 200\n",
      "Best val acc 107: 0.49999961256980896\n",
      "108 of 200\n",
      "Best val acc 108: 0.49999961256980896\n",
      "109 of 200\n",
      "Best val acc 109: 0.49999961256980896\n",
      "110 of 200\n",
      "Best val acc 110: 0.49999961256980896\n",
      "111 of 200\n",
      "Best val acc 111: 0.49999961256980896\n",
      "112 of 200\n",
      "Best val acc 112: 0.49999961256980896\n",
      "113 of 200\n",
      "Best val acc 113: 0.49999961256980896\n",
      "114 of 200\n",
      "Best val acc 114: 0.49999961256980896\n",
      "115 of 200\n",
      "Best val acc 115: 0.49999961256980896\n",
      "116 of 200\n",
      "Best val acc 116: 0.49999961256980896\n",
      "117 of 200\n",
      "Best val acc 117: 0.49999961256980896\n",
      "118 of 200\n",
      "Best val acc 118: 0.49999961256980896\n",
      "119 of 200\n",
      "Best val acc 119: 0.49999961256980896\n",
      "120 of 200\n",
      "Best val acc 120: 0.49999961256980896\n",
      "121 of 200\n",
      "Best val acc 121: 0.49999961256980896\n",
      "122 of 200\n",
      "Best val acc 122: 0.49999961256980896\n",
      "123 of 200\n",
      "Best val acc 123: 0.49999961256980896\n",
      "124 of 200\n",
      "Best val acc 124: 0.49999961256980896\n",
      "125 of 200\n",
      "Best val acc 125: 0.49999961256980896\n",
      "126 of 200\n",
      "Best val acc 126: 0.49999961256980896\n",
      "127 of 200\n",
      "Best val acc 127: 0.49999961256980896\n",
      "128 of 200\n",
      "Best val acc 128: 0.49999961256980896\n",
      "129 of 200\n",
      "Best val acc 129: 0.49999961256980896\n",
      "130 of 200\n",
      "Best val acc 130: 0.49999961256980896\n",
      "131 of 200\n",
      "Best val acc 131: 0.49999961256980896\n",
      "132 of 200\n",
      "Best val acc 132: 0.49999961256980896\n",
      "133 of 200\n",
      "Best val acc 133: 0.49999961256980896\n",
      "134 of 200\n",
      "Best val acc 134: 0.49999961256980896\n",
      "135 of 200\n",
      "Best val acc 135: 0.49999961256980896\n",
      "136 of 200\n",
      "Best val acc 136: 0.49999961256980896\n",
      "137 of 200\n",
      "Best val acc 137: 0.49999961256980896\n",
      "138 of 200\n",
      "Best val acc 138: 0.49999961256980896\n",
      "139 of 200\n",
      "Best val acc 139: 0.49999961256980896\n",
      "140 of 200\n",
      "Best val acc 140: 0.49999961256980896\n",
      "141 of 200\n",
      "Best val acc 141: 0.49999961256980896\n",
      "142 of 200\n",
      "Best val acc 142: 0.49999961256980896\n",
      "143 of 200\n",
      "Best val acc 143: 0.49999961256980896\n",
      "144 of 200\n",
      "Best val acc 144: 0.49999961256980896\n",
      "145 of 200\n",
      "Best val acc 145: 0.49999961256980896\n",
      "146 of 200\n",
      "Best val acc 146: 0.49999961256980896\n",
      "147 of 200\n",
      "Best val acc 147: 0.49999961256980896\n",
      "148 of 200\n",
      "Best val acc 148: 0.49999961256980896\n",
      "149 of 200\n",
      "Best val acc 149: 0.49999961256980896\n",
      "150 of 200\n",
      "Best val acc 150: 0.49999961256980896\n",
      "151 of 200\n",
      "Best val acc 151: 0.49999961256980896\n",
      "152 of 200\n",
      "Best val acc 152: 0.49999961256980896\n",
      "153 of 200\n",
      "Best val acc 153: 0.49999961256980896\n",
      "154 of 200\n",
      "Best val acc 154: 0.49999961256980896\n",
      "155 of 200\n",
      "Best val acc 155: 0.49999961256980896\n",
      "156 of 200\n",
      "Best val acc 156: 0.49999961256980896\n",
      "157 of 200\n",
      "Best val acc 157: 0.49999961256980896\n",
      "158 of 200\n",
      "Best val acc 158: 0.49999961256980896\n",
      "159 of 200\n",
      "Best val acc 159: 0.49999961256980896\n",
      "160 of 200\n",
      "Best val acc 160: 0.49999961256980896\n",
      "161 of 200\n",
      "Best val acc 161: 0.49999961256980896\n",
      "162 of 200\n",
      "Best val acc 162: 0.49999961256980896\n",
      "163 of 200\n",
      "Best val acc 163: 0.49999961256980896\n",
      "164 of 200\n",
      "Best val acc 164: 0.49999961256980896\n",
      "165 of 200\n",
      "Best val acc 165: 0.49999961256980896\n",
      "166 of 200\n",
      "Best val acc 166: 0.49999961256980896\n",
      "167 of 200\n",
      "Best val acc 167: 0.49999961256980896\n",
      "168 of 200\n",
      "Best val acc 168: 0.49999961256980896\n",
      "169 of 200\n",
      "Best val acc 169: 0.49999961256980896\n",
      "170 of 200\n",
      "Best val acc 170: 0.49999961256980896\n",
      "171 of 200\n",
      "Best val acc 171: 0.49999961256980896\n",
      "172 of 200\n",
      "Best val acc 172: 0.49999961256980896\n",
      "173 of 200\n",
      "Best val acc 173: 0.49999961256980896\n",
      "174 of 200\n",
      "Best val acc 174: 0.49999961256980896\n",
      "175 of 200\n",
      "Best val acc 175: 0.49999961256980896\n",
      "176 of 200\n",
      "Best val acc 176: 0.49999961256980896\n",
      "177 of 200\n",
      "Best val acc 177: 0.49999961256980896\n",
      "178 of 200\n",
      "Best val acc 178: 0.49999961256980896\n",
      "179 of 200\n",
      "Best val acc 179: 0.49999961256980896\n",
      "180 of 200\n",
      "Best val acc 180: 0.49999961256980896\n",
      "181 of 200\n",
      "Best val acc 181: 0.49999961256980896\n",
      "182 of 200\n",
      "Best val acc 182: 0.49999961256980896\n",
      "183 of 200\n",
      "Best val acc 183: 0.49999961256980896\n",
      "184 of 200\n",
      "Best val acc 184: 0.49999961256980896\n",
      "185 of 200\n",
      "Best val acc 185: 0.49999961256980896\n",
      "186 of 200\n",
      "Best val acc 186: 0.49999961256980896\n",
      "187 of 200\n",
      "Best val acc 187: 0.49999961256980896\n",
      "188 of 200\n",
      "Best val acc 188: 0.49999961256980896\n",
      "189 of 200\n",
      "Best val acc 189: 0.49999961256980896\n",
      "190 of 200\n",
      "Best val acc 190: 0.49999961256980896\n",
      "191 of 200\n",
      "Best val acc 191: 0.49999961256980896\n",
      "192 of 200\n",
      "Best val acc 192: 0.49999961256980896\n",
      "193 of 200\n",
      "Best val acc 193: 0.49999961256980896\n",
      "194 of 200\n",
      "Best val acc 194: 0.49999961256980896\n",
      "195 of 200\n",
      "Best val acc 195: 0.49999961256980896\n",
      "196 of 200\n",
      "Best val acc 196: 0.49999961256980896\n",
      "197 of 200\n",
      "Best val acc 197: 0.49999961256980896\n",
      "198 of 200\n",
      "Best val acc 198: 0.49999961256980896\n",
      "199 of 200\n",
      "Best val acc 199: 0.49999961256980896\n",
      "cuda\n",
      "in_features=2/h_features=[5/5]/out_features=4/activation=ReLU()/norm_edges=both/norm_nodes=gn/dropout=0.2/lr=0.01///lr=0.01/optimizer_name=adamw/scheduler_mode=max_val_acc/label_smoothing=0.0/use_edge_weight=True\n",
      "0 of 200\n",
      "Best val acc 0: 0.41666632890701294\n",
      "1 of 200\n",
      "Best val acc 1: 0.41666632890701294\n",
      "2 of 200\n",
      "Best val acc 2: 0.41666632890701294\n",
      "3 of 200\n",
      "Best val acc 3: 0.41666632890701294\n",
      "4 of 200\n",
      "Best val acc 4: 0.41666632890701294\n",
      "5 of 200\n",
      "Best val acc 5: 0.49999961256980896\n",
      "6 of 200\n",
      "7 of 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m list_gcn_model:\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGCN\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdict_model\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../models/logs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../models/saved\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43madamw\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscheduler_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax_val_acc\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdebug_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_validate\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cpu\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_edge_weight\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\systemic-risk-predictor\\models\\train.py:130\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dict_model, dataset, log_dir, save_path, lr, optimizer_name, n_epochs, scheduler_mode, debug_mode, steps_validate, use_cpu, label_smoothing, use_edge_weight)\u001B[0m\n\u001B[0;32m    127\u001B[0m train_mask \u001B[38;5;241m=\u001B[39m g\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    129\u001B[0m \u001B[38;5;66;03m# Compute loss on training and update parameters\u001B[39;00m\n\u001B[1;32m--> 130\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medge_weight\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_edge_weight\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m loss_train \u001B[38;5;241m=\u001B[39m loss(logits[train_mask], labels[train_mask])\n\u001B[0;32m    133\u001B[0m \u001B[38;5;66;03m# Do back propagation\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\vibal\\pycharmprojects\\systemic-risk-predictor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\systemic-risk-predictor\\models\\models.py:67\u001B[0m, in \u001B[0;36mGCN.forward\u001B[1;34m(self, g, feats, edge_weight)\u001B[0m\n\u001B[0;32m     65\u001B[0m     h \u001B[38;5;241m=\u001B[39m n(g,h)\n\u001B[0;32m     66\u001B[0m     h \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(h, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[1;32m---> 67\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[43ml\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medge_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m h\n",
      "File \u001B[1;32mc:\\users\\vibal\\pycharmprojects\\systemic-risk-predictor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\vibal\\pycharmprojects\\systemic-risk-predictor\\venv\\lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:407\u001B[0m, in \u001B[0;36mGraphConv.forward\u001B[1;34m(self, graph, feat, weight, edge_weight)\u001B[0m\n\u001B[0;32m    405\u001B[0m feat_src, feat_dst \u001B[38;5;241m=\u001B[39m expand_as_pair(feat, graph)\n\u001B[0;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_norm \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m--> 407\u001B[0m     degs \u001B[38;5;241m=\u001B[39m \u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_degrees\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    408\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_norm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    409\u001B[0m         norm \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mpow(degs, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n",
      "File \u001B[1;32mc:\\users\\vibal\\pycharmprojects\\systemic-risk-predictor\\venv\\lib\\site-packages\\dgl\\heterograph.py:3580\u001B[0m, in \u001B[0;36mDGLHeteroGraph.out_degrees\u001B[1;34m(self, u, etype)\u001B[0m\n\u001B[0;32m   3578\u001B[0m     u \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msrcnodes(srctype)\n\u001B[0;32m   3579\u001B[0m u_tensor \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mprepare_tensor(\u001B[38;5;28mself\u001B[39m, u, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 3580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m F\u001B[38;5;241m.\u001B[39mas_scalar(F\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_nodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mntype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msrctype\u001B[49m\u001B[43m)\u001B[49m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(u_tensor):\n\u001B[0;32m   3581\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DGLError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu contains invalid node IDs\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   3582\u001B[0m deg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39mout_degrees(etid, utils\u001B[38;5;241m.\u001B[39mprepare_tensor(\u001B[38;5;28mself\u001B[39m, u, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32mc:\\users\\vibal\\pycharmprojects\\systemic-risk-predictor\\venv\\lib\\site-packages\\dgl\\heterograph.py:2730\u001B[0m, in \u001B[0;36mDGLHeteroGraph.has_nodes\u001B[1;34m(self, vid, ntype)\u001B[0m\n\u001B[0;32m   2683\u001B[0m \u001B[38;5;124;03m\"\"\"Return whether the graph contains the given nodes.\u001B[39;00m\n\u001B[0;32m   2684\u001B[0m \n\u001B[0;32m   2685\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2727\u001B[0m \u001B[38;5;124;03mtensor([False,  True,  True])\u001B[39;00m\n\u001B[0;32m   2728\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2729\u001B[0m vid_tensor \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mprepare_tensor(\u001B[38;5;28mself\u001B[39m, vid, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2730\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(vid_tensor) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_scalar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvid_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(vid_tensor):\n\u001B[0;32m   2731\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DGLError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAll IDs must be non-negative integers.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   2732\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39mhas_nodes(\n\u001B[0;32m   2733\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_ntype_id(ntype), vid_tensor)\n",
      "File \u001B[1;32mc:\\users\\vibal\\pycharmprojects\\systemic-risk-predictor\\venv\\lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:49\u001B[0m, in \u001B[0;36mas_scalar\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mas_scalar\u001B[39m(data):\n\u001B[1;32m---> 49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for d in list_gcn_model:\n",
    "    train(\n",
    "        model = models.GCN(**d),\n",
    "        dict_model = d,\n",
    "        dataset = dataset,\n",
    "        log_dir = '../models/logs',\n",
    "        save_path = '../models/saved',\n",
    "        lr = d['lr'],\n",
    "        optimizer_name = \"adamw\",\n",
    "        n_epochs = 200,\n",
    "        scheduler_mode = 'max_val_acc',\n",
    "        debug_mode = False,\n",
    "        steps_validate = 1,\n",
    "        use_cpu = False,\n",
    "        label_smoothing = 0.0,\n",
    "        use_edge_weight = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "in_features=2/h_features=[5/5]/out_features=4/num_heads=[2/2]/norm_nodes=None/activation=None/negative_slope=0.2/feat_drop=0.2/attn_drop=0.2/residual=False/lr=0.005///lr=0.005/optimizer_name=adamw/scheduler_mode=max_val_acc/label_smoothing=0.0/use_edge_weight=True\n",
      "0 of 200\n",
      "Best val acc 0: 0.08333326876163483\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 123] El nombre de archivo, el nombre de directorio o la sintaxis de la etiqueta del volumen no son correctos: 'models\\\\saved\\\\in_features=2_h_features=[5_5]_out_features=4_num_heads=[2_2]_norm_nodes=None_activation=None_negative_slope=0.2_feat_drop=0.2_attn_drop=0.2_residual=False_lr=0.005___lr=0.005_optimizer_name=adamw_scheduler_mode=max_val_acc_label_smoothing=0.0_use_edge_weight=True'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\notebooks\\models_training.ipynb Cell 8'\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=0'>1</a>\u001B[0m \u001B[39mfor\u001B[39;00m d \u001B[39min\u001B[39;00m list_gat_model:\n\u001B[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=1'>2</a>\u001B[0m     train(\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=2'>3</a>\u001B[0m         model \u001B[39m=\u001B[39;49m models\u001B[39m.\u001B[39;49mGAT(\u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49md),\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=3'>4</a>\u001B[0m         dict_model \u001B[39m=\u001B[39;49m d,\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=4'>5</a>\u001B[0m         dataset \u001B[39m=\u001B[39;49m dataset,\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=5'>6</a>\u001B[0m         log_dir \u001B[39m=\u001B[39;49m \u001B[39m'\u001B[39;49m\u001B[39m./models/logs\u001B[39;49m\u001B[39m'\u001B[39;49m,\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=6'>7</a>\u001B[0m         save_path \u001B[39m=\u001B[39;49m \u001B[39m'\u001B[39;49m\u001B[39m./models/saved\u001B[39;49m\u001B[39m'\u001B[39;49m,\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=7'>8</a>\u001B[0m         lr \u001B[39m=\u001B[39;49m d[\u001B[39m'\u001B[39;49m\u001B[39mlr\u001B[39;49m\u001B[39m'\u001B[39;49m],\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=8'>9</a>\u001B[0m         optimizer_name \u001B[39m=\u001B[39;49m \u001B[39m\"\u001B[39;49m\u001B[39madamw\u001B[39;49m\u001B[39m\"\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=9'>10</a>\u001B[0m         n_epochs \u001B[39m=\u001B[39;49m \u001B[39m200\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=10'>11</a>\u001B[0m         scheduler_mode \u001B[39m=\u001B[39;49m \u001B[39m'\u001B[39;49m\u001B[39mmax_val_acc\u001B[39;49m\u001B[39m'\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=11'>12</a>\u001B[0m         debug_mode \u001B[39m=\u001B[39;49m \u001B[39mFalse\u001B[39;49;00m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=12'>13</a>\u001B[0m         steps_validate \u001B[39m=\u001B[39;49m \u001B[39m1\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=13'>14</a>\u001B[0m         use_cpu \u001B[39m=\u001B[39;49m \u001B[39mFalse\u001B[39;49;00m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=14'>15</a>\u001B[0m         label_smoothing \u001B[39m=\u001B[39;49m \u001B[39m0.0\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=15'>16</a>\u001B[0m         use_edge_weight \u001B[39m=\u001B[39;49m \u001B[39mTrue\u001B[39;49;00m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/models_training.ipynb#ch0000010?line=16'>17</a>\u001B[0m     )\n",
      "File \u001B[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\notebooks\\..\\models\\train.py:202\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dict_model, dataset, log_dir, save_path, lr, optimizer_name, n_epochs, scheduler_mode, debug_mode, steps_validate, use_cpu, label_smoothing, use_edge_weight)\u001B[0m\n\u001B[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/../models/train.py?line=199'>200</a>\u001B[0m dict_model[\u001B[39m\"\u001B[39m\u001B[39mepoch\u001B[39m\u001B[39m\"\u001B[39m] \u001B[39m=\u001B[39m epoch\n\u001B[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/../models/train.py?line=200'>201</a>\u001B[0m name_path \u001B[39m=\u001B[39m name_model\u001B[39m.\u001B[39mreplace(\u001B[39m'\u001B[39m\u001B[39m/\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39m_\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/../models/train.py?line=201'>202</a>\u001B[0m save_model(model, save_path, name_path, param_dicts\u001B[39m=\u001B[39;49mdict_model)\n",
      "File \u001B[1;32mc:\\Users\\vibal\\PycharmProjects\\systemic-risk-predictor\\notebooks\\..\\models\\models.py:247\u001B[0m, in \u001B[0;36msave_model\u001B[1;34m(model, folder, model_name, param_dicts)\u001B[0m\n\u001B[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/../models/models.py?line=244'>245</a>\u001B[0m \u001B[39m# create folder if it does not exist\u001B[39;00m\n\u001B[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/../models/models.py?line=245'>246</a>\u001B[0m folder_path \u001B[39m=\u001B[39m \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m{\u001B[39;00mfolder\u001B[39m}\u001B[39;00m\u001B[39m/\u001B[39m\u001B[39m{\u001B[39;00mmodel_name\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m\n\u001B[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/../models/models.py?line=246'>247</a>\u001B[0m pathlib\u001B[39m.\u001B[39;49mPath(folder_path)\u001B[39m.\u001B[39;49mmkdir(parents\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m, exist_ok\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m)\n\u001B[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/../models/models.py?line=247'>248</a>\u001B[0m \u001B[39m# save model\u001B[39;00m\n\u001B[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/systemic-risk-predictor/notebooks/../models/models.py?line=248'>249</a>\u001B[0m torch\u001B[39m.\u001B[39msave(model\u001B[39m.\u001B[39mstate_dict(), \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m{\u001B[39;00mfolder_path\u001B[39m}\u001B[39;00m\u001B[39m/\u001B[39m\u001B[39m{\u001B[39;00mmodel_name\u001B[39m}\u001B[39;00m\u001B[39m.th\u001B[39m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\pathlib.py:1313\u001B[0m, in \u001B[0;36mPath.mkdir\u001B[1;34m(self, mode, parents, exist_ok)\u001B[0m\n\u001B[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/pathlib.py?line=1308'>1309</a>\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/pathlib.py?line=1309'>1310</a>\u001B[0m \u001B[39mCreate a new directory at this given path.\u001B[39;00m\n\u001B[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/pathlib.py?line=1310'>1311</a>\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/pathlib.py?line=1311'>1312</a>\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/pathlib.py?line=1312'>1313</a>\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_accessor\u001B[39m.\u001B[39;49mmkdir(\u001B[39mself\u001B[39;49m, mode)\n\u001B[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/pathlib.py?line=1313'>1314</a>\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/pathlib.py?line=1314'>1315</a>\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m parents \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mparent \u001B[39m==\u001B[39m \u001B[39mself\u001B[39m:\n",
      "\u001B[1;31mOSError\u001B[0m: [WinError 123] El nombre de archivo, el nombre de directorio o la sintaxis de la etiqueta del volumen no son correctos: 'models\\\\saved\\\\in_features=2_h_features=[5_5]_out_features=4_num_heads=[2_2]_norm_nodes=None_activation=None_negative_slope=0.2_feat_drop=0.2_attn_drop=0.2_residual=False_lr=0.005___lr=0.005_optimizer_name=adamw_scheduler_mode=max_val_acc_label_smoothing=0.0_use_edge_weight=True'"
     ]
    }
   ],
   "source": [
    "for d in list_gat_model:\n",
    "    train(\n",
    "        model = models.GAT(**d),\n",
    "        dict_model = d,\n",
    "        dataset = dataset,\n",
    "        log_dir = '../models/logs',\n",
    "        save_path = '../models/saved',\n",
    "        lr = d['lr'],\n",
    "        optimizer_name = \"adamw\",\n",
    "        n_epochs = 200,\n",
    "        scheduler_mode = 'max_val_acc',\n",
    "        debug_mode = False,\n",
    "        steps_validate = 1,\n",
    "        use_cpu = False,\n",
    "        label_smoothing = 0.0,\n",
    "        use_edge_weight = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for d in list_sage_model:\n",
    "    train(\n",
    "        model = models.GraphSAGE(**d),\n",
    "        dict_model = d,\n",
    "        dataset = dataset,\n",
    "        log_dir = './models/logs',\n",
    "        save_path = './models/saved',\n",
    "        lr = d['lr'],\n",
    "        optimizer_name = \"adamw\",\n",
    "        n_epochs = 200,\n",
    "        scheduler_mode = 'max_val_acc',\n",
    "        debug_mode = False,\n",
    "        steps_validate = 1,\n",
    "        use_cpu = False,\n",
    "        label_smoothing = 0.0,\n",
    "        use_edge_weight = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GCN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = ContagionDataset(\n",
    "    raw_dir='./data',\n",
    "    drop_edges=0,\n",
    "    sets_lengths=(0.8, 0.1, 0.1),\n",
    ")\n",
    "\n",
    "gcn_model = dict(\n",
    "    in_features=[dataset.node_features],\n",
    "    h_features=[[5, 5], [5, 10], [10, 5]],\n",
    "    out_features=[dataset.num_classes],\n",
    "    activation=[torch.nn.ReLU()],\n",
    "    norm_edges=['both'],\n",
    "    norm_nodes=[None, 'bn', 'gn'],\n",
    "    dropout=[0.2, 0.5, 0.0],\n",
    "    # other\n",
    "    lr=[1e-2, 1, 1e-3],\n",
    "    label_smoothing=[0.0, 0.2, 0.4],\n",
    ")\n",
    "list_gcn_model = [dict(zip(gcn_model.keys(), k)) for k in itertools.product(*gcn_model.values())]\n",
    "\n",
    "for d in list_gcn_model:\n",
    "    lr = d.pop('lr')\n",
    "    ls = d.pop('label_smoothing')\n",
    "    train(\n",
    "        model=GCN(**d),\n",
    "        dict_model=d,\n",
    "        dataset=dataset,\n",
    "        log_dir='./models/logs',\n",
    "        save_path='./models/saved',\n",
    "        lr=lr,\n",
    "        optimizer_name=\"adamw\",\n",
    "        n_epochs=100,\n",
    "        scheduler_mode='max_val_acc',\n",
    "        debug_mode=False,\n",
    "        steps_validate=1,\n",
    "        use_cpu=False,\n",
    "        label_smoothing=ls,\n",
    "        use_edge_weight=True,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GraphSAGE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37be9487e307834247f9cc00a1ec46ceeb3f522b7edf17e3b2d74c6ce713e314"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}